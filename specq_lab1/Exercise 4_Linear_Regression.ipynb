{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Python: Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the exercise, we're tasked with implementing linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries and examining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the CSV file using Panda library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Population   Profit\n",
       "0      6.1101  17.5920\n",
       "1      5.5277   9.1302\n",
       "2      8.5186  13.6620\n",
       "3      7.0032  11.8540\n",
       "4      5.8598   6.8233"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd() + '/data/ex1data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Population', 'Profit'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.159800</td>\n",
       "      <td>5.839135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.869884</td>\n",
       "      <td>5.510262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.026900</td>\n",
       "      <td>-2.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.707700</td>\n",
       "      <td>1.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.589400</td>\n",
       "      <td>4.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.578100</td>\n",
       "      <td>7.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.203000</td>\n",
       "      <td>24.147000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Population     Profit\n",
       "count   97.000000  97.000000\n",
       "mean     8.159800   5.839135\n",
       "std      3.869884   5.510262\n",
       "min      5.026900  -2.680700\n",
       "25%      5.707700   1.986900\n",
       "50%      6.589400   4.562300\n",
       "75%      8.578100   7.046700\n",
       "max     22.203000  24.147000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it to get a better idea of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Population', ylabel='Profit'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement linear regression using gradient descent to minimize the cost function.  The equations implemented in the following code samples are detailed in \"ex1.pdf\" in the \"exercises\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll create a function to compute the cost of a given solution (characterized by the parameters theta). The cost function is the Mean Sqaured error in matrix form: \n",
    "\n",
    "$$ MSE(\\theta) = \\frac{1}{N}\\sum_n^N [ y_n-x_n^T*\\theta]^2 $$\n",
    "\n",
    "where $\\theta$ and $x_n$ are vectors\n",
    "\n",
    "__Hint__: Use the matrix form of the cost function and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(x, y, theta):\n",
    "    # Compute the cost function\n",
    "    SE = (y-np.dot(x, theta)) * (y-np.dot(x, theta)) \n",
    "    return np.mean(SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column of ones to the training set so we can use a vectorized solution to computing the cost and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.insert(0, 'Ones', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some variable initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]\n",
    "X = data.iloc[:,0:cols-1]\n",
    "y = data.iloc[:,cols-1:cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to make sure X (training set) and y (target variable) look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.8598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ones  Population\n",
       "0     1      6.1101\n",
       "1     1      5.5277\n",
       "2     1      8.5186\n",
       "3     1      7.0032\n",
       "4     1      5.8598"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Profit\n",
       "0  17.5920\n",
       "1   9.1302\n",
       "2  13.6620\n",
       "3  11.8540\n",
       "4   6.8233"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert X and Y to numpy array for better manipulation. Initiliaze Theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X.values)\n",
    "y = np.array(y.values).flatten()\n",
    "theta = np.array([0,0])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the shape of our matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97, 2), (2,), (97,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, theta.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the cost for our initial solution (0 values for theta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.14546775491135"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good.  Now we need to define a function to perform gradient descent on the parameters theta using the update rules. Write first a function that computes the gradient of a matrix and then use it in the gradientDescent function.\n",
    "\n",
    "The gradient descent formula is:\n",
    "\n",
    "$$\\theta^{t+1} = \\theta^{t} - \\alpha*\\nabla MSE(\\theta^{t})$$\n",
    "\n",
    "where $\\nabla MSE(\\theta^{t})$ is the gradient of the cost function at $\\theta^{t}$\n",
    "\n",
    "__Hint__: Use the matrix form of the gradient and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    return -2*np.dot(tx.T,(y-np.dot(tx,w)))/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha,max_iters):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [theta]\n",
    "    cost = np.zeros(max_iters)\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient(y,X,theta)\n",
    "        theta = theta - alpha * grad\n",
    "        loss = computeCost(X,y,theta)\n",
    "        # store w and loss\n",
    "        ws.append(theta)\n",
    "        cost[n_iter] = loss\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=theta[0], w1=theta[1]))\n",
    "\n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize some additional variables - the learning rate alpha, and the number of iterations to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iters = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the gradient descent algorithm to fit our parameters theta to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=33.53928474333491, w0=0.11678270103092779, w1=1.3065769949111339\n",
      "Gradient Descent(1/999): loss=20.815159503537938, w0=0.018001608779719666, w1=0.46688514041747775\n",
      "Gradient Descent(2/999): loss=15.518366960604673, w0=0.058230490259482424, w1=1.010398519281903\n",
      "Gradient Descent(3/999): loss=13.306575069293501, w0=0.008955584732491167, w1=0.6624640652822505\n",
      "Gradient Descent(4/999): loss=12.376211646075983, w0=0.017447688470967015, w1=0.8890358067492414\n",
      "Gradient Descent(5/999): loss=11.978169038728408, w0=-0.011205651785773693, w1=0.7453450091050499\n",
      "Gradient Descent(6/999): loss=11.801307545851571, w0=-0.015836161825038105, w1=0.8402702714870819\n",
      "Gradient Descent(7/999): loss=11.716395840326452, w0=-0.035865484783215315, w1=0.7814054019591588\n",
      "Gradient Descent(8/999): loss=11.66975757040281, w0=-0.04588771003475004, w1=0.8216458872863021\n",
      "Gradient Descent(9/999): loss=11.63909786844649, w0=-0.06227657702470256, w1=0.7980072763515389\n",
      "Gradient Descent(10/999): loss=11.615156054554554, w0=-0.07447993992474641, w1=0.8155287856125966\n",
      "Gradient Descent(11/999): loss=11.594085154771328, w0=-0.08929867579215696, w1=0.8065154361246225\n",
      "Gradient Descent(12/999): loss=11.57428676210951, w0=-0.10235009435917988, w1=0.8145948887823523\n",
      "Gradient Descent(13/999): loss=11.555096407741408, w0=-0.11645901891079323, w1=0.8116502947008781\n",
      "Gradient Descent(14/999): loss=11.536237534751159, w0=-0.12980521899565406, w1=0.8158022520353791\n",
      "Gradient Descent(15/999): loss=11.517594708120667, w0=-0.14356207790797887, w1=0.8153725418897819\n",
      "Gradient Descent(16/999): loss=11.499119418305693, w0=-0.15697367266513632, w1=0.8178874980936855\n",
      "Gradient Descent(17/999): loss=11.480790957069816, w0=-0.17052746631980284, w1=0.8184966258951272\n",
      "Gradient Descent(18/999): loss=11.462600167377458, w0=-0.1839095913220601, w1=0.8203259705114381\n",
      "Gradient Descent(19/999): loss=11.444542693942985, w0=-0.1973226155482757, w1=0.8213609071086568\n",
      "Gradient Descent(20/999): loss=11.42661617984241, w0=-0.2106362768028867, w1=0.8228998372158082\n",
      "Gradient Descent(21/999): loss=11.408819102107842, w0=-0.22393481207017218, w1=0.824106004016073\n",
      "Gradient Descent(22/999): loss=11.391150288067553, w0=-0.23716421822924796, w1=0.8255187039129651\n",
      "Gradient Descent(23/999): loss=11.373608714431438, w0=-0.25035958323751545, w1=0.826790403144767\n",
      "Gradient Descent(24/999): loss=11.356193423813906, w0=-0.2634985771734507, w1=0.8281451065334089\n",
      "Gradient Descent(25/999): loss=11.338903490034212, w0=-0.27659587340488007, w1=0.8294384772326225\n",
      "Gradient Descent(26/999): loss=11.321738003676488, w0=-0.2896422966363097, w1=0.8307635647986902\n",
      "Gradient Descent(27/999): loss=11.304696066064789, w0=-0.3026440403935427, w1=0.8320604294011863\n",
      "Gradient Descent(28/999): loss=11.28777678673345, w0=-0.3155973923913, w1=0.8333677287388969\n",
      "Gradient Descent(29/999): loss=11.27097928234944, w0=-0.3285050233718192, w1=0.8346605768984672\n",
      "Gradient Descent(30/999): loss=11.254302676237959, w0=-0.3413654893809772, w1=0.8359550400503489\n",
      "Gradient Descent(31/999): loss=11.237746098158803, w0=-0.3541799972784866, w1=0.8372407917250402\n",
      "Gradient Descent(32/999): loss=11.221308684187079, w0=-0.3669480445483487, w1=0.8385245147143747\n",
      "Gradient Descent(33/999): loss=11.204989576637416, w0=-0.37967022932978095, w1=0.8398019296894985\n",
      "Gradient Descent(34/999): loss=11.1887879240065, w0=-0.3923464394298649, w1=0.8410758220203125\n",
      "Gradient Descent(35/999): loss=11.17270288092331, w0=-0.4049770194607667, w1=0.842344423965439\n",
      "Gradient Descent(36/999): loss=11.156733608102838, w0=-0.4175620186540873, w1=0.8436089020451878\n",
      "Gradient Descent(37/999): loss=11.140879272301378, w0=-0.4301016756282442, w1=0.8448685316272173\n",
      "Gradient Descent(38/999): loss=11.125139046272661, w0=-0.44259610597218685, w1=0.8461238067793512\n",
      "Gradient Descent(39/999): loss=11.109512108724545, w0=-0.4550455035929783, w1=0.8473744360516899\n",
      "Gradient Descent(40/999): loss=11.093997644276083, w0=-0.46745001095608246, w1=0.8486206341426101\n",
      "Gradient Descent(41/999): loss=11.078594843414963, w0=-0.4798098027155704, w1=0.8498622894543948\n",
      "Gradient Descent(42/999): loss=11.063302902455254, w0=-0.49212503182013057, w1=0.8510995005820563\n",
      "Gradient Descent(43/999): loss=11.048121023495467, w0=-0.5043958642497894, w1=0.8523322305607661\n",
      "Gradient Descent(44/999): loss=11.033048414376907, w0=-0.5166224566324606, w1=0.8535605296946278\n",
      "Gradient Descent(45/999): loss=11.018084288642372, w0=-0.5288049706729281, w1=0.8547843919497466\n",
      "Gradient Descent(46/999): loss=11.003227865495083, w0=-0.5409435638571726, w1=0.8560038475054581\n",
      "Gradient Descent(47/999): loss=10.988478369757956, w0=-0.553038395446602, w1=0.857218903107842\n",
      "Gradient Descent(48/999): loss=10.973835031833165, w0=-0.5650896226183295, w1=0.8584295805106528\n",
      "Gradient Descent(49/999): loss=10.959297087661964, w0=-0.5770974029560516, w1=0.8596358917014932\n",
      "Gradient Descent(50/999): loss=10.944863778684843, w0=-0.5890618928481196, w1=0.8608378548682328\n",
      "Gradient Descent(51/999): loss=10.930534351801933, w0=-0.6009832485233055, w1=0.8620354841094084\n",
      "Gradient Descent(52/999): loss=10.916308059333717, w0=-0.6128616253866306, w1=0.8632287960659379\n",
      "Gradient Descent(53/999): loss=10.902184158982017, w0=-0.6246971784507469, w1=0.8644178056474904\n",
      "Gradient Descent(54/999): loss=10.888161913791253, w0=-0.6364900620612519, w1=0.8656025287870628\n",
      "Gradient Descent(55/999): loss=10.874240592110011, w0=-0.6482404300770326, w1=0.8667829806660607\n",
      "Gradient Descent(56/999): loss=10.860419467552845, w0=-0.6599484357573425, w1=0.8679591768585191\n",
      "Gradient Descent(57/999): loss=10.84669781896239, w0=-0.6716142318378707, w1=0.8691311325940132\n",
      "Gradient Descent(58/999): loss=10.833074930371732, w0=-0.683237970484998, w1=0.8702988632330222\n",
      "Gradient Descent(59/999): loss=10.819550090967038, w0=-0.6948198033285465, w1=0.8714623839608986\n",
      "Gradient Descent(60/999): loss=10.806122595050482, w0=-0.7063598814439306, w1=0.872621709985424\n",
      "Gradient Descent(61/999): loss=10.792791742003423, w0=-0.7178583553669053, w1=0.8737768564098157\n",
      "Gradient Descent(62/999): loss=10.779556836249832, w0=-0.7293153750872957, w1=0.8749278383148954\n",
      "Gradient Descent(63/999): loss=10.766417187220013, w0=-0.7407310900562596, w1=0.8760746707064573\n",
      "Gradient Descent(64/999): loss=10.753372109314567, w0=-0.7521056491848176, w1=0.8772173685495103\n",
      "Gradient Descent(65/999): loss=10.740420921868619, w0=-0.7634392008479993, w1=0.8783559467465318\n",
      "Gradient Descent(66/999): loss=10.72756294911631, w0=-0.7747318928853585, w1=0.8794904201518022\n",
      "Gradient Descent(67/999): loss=10.71479752015551, w0=-0.785983872603817, w1=0.8806208035624858\n",
      "Gradient Descent(68/999): loss=10.702123968912844, w0=-0.7971952867789962, w1=0.8817471117246971\n",
      "Gradient Descent(69/999): loss=10.689541634108924, w0=-0.8083662816575121, w1=0.8828693593299076\n",
      "Gradient Descent(70/999): loss=10.67704985922384, w0=-0.8194970029586377, w1=0.8839875610175774\n",
      "Gradient Descent(71/999): loss=10.664647992462907, w0=-0.8305875958763617, w1=0.8851017313737736\n",
      "Gradient Descent(72/999): loss=10.652335386722669, w0=-0.8416382050811809, w1=0.8862118849323741\n",
      "Gradient Descent(73/999): loss=10.640111399557108, w0=-0.8526489747220533, w1=0.8873180361746037\n",
      "Gradient Descent(74/999): loss=10.627975393144158, w0=-0.8636200484282349, w1=0.8884201995296449\n",
      "Gradient Descent(75/999): loss=10.61592673425238, w0=-0.8745515693111823, w1=0.8895183893745532\n",
      "Gradient Descent(76/999): loss=10.603964794207966, w0=-0.8854436799664004, w1=0.8906126200346213\n",
      "Gradient Descent(77/999): loss=10.592088948861903, w0=-0.8962965224753145, w1=0.8917029057834517\n",
      "Gradient Descent(78/999): loss=10.580298578557432, w0=-0.9071102384071166, w1=0.892789260843217\n",
      "Gradient Descent(79/999): loss=10.56859306809769, w0=-0.9178849688206161, w1=0.8938716993847968\n",
      "Gradient Descent(80/999): loss=10.556971806713634, w0=-0.9286208542660772, w1=0.8949502355279948\n",
      "Gradient Descent(81/999): loss=10.545434188032146, w0=-0.9393180347870544, w1=0.896024883341702\n",
      "Gradient Descent(82/999): loss=10.533979610044433, w0=-0.949976649922218, w1=0.897095656844093\n",
      "Gradient Descent(83/999): loss=10.52260747507457, w0=-0.9605968387071744, w1=0.8981625700028019\n",
      "Gradient Descent(84/999): loss=10.511317189748349, w0=-0.9711787396762803, w1=0.8992256367351077\n",
      "Gradient Descent(85/999): loss=10.500108164962297, w0=-0.9817224908644495, w1=0.9002848709081145\n",
      "Gradient Descent(86/999): loss=10.488979815852952, w0=-0.9922282298089533, w1=0.9013402863389334\n",
      "Gradient Descent(87/999): loss=10.477931561766338, w0=-1.002696093551215, w1=0.9023918967948614\n",
      "Gradient Descent(88/999): loss=10.466962826227666, w0=-1.0131262186385972, w1=0.9034397159935627\n",
      "Gradient Descent(89/999): loss=10.456073036911265, w0=-1.0235187411261828, w1=0.9044837576032456\n",
      "Gradient Descent(90/999): loss=10.445261625610708, w0=-1.0338737965785505, w1=0.9055240352428429\n",
      "Gradient Descent(91/999): loss=10.434528028209176, w0=-1.0441915200715426, w1=0.9065605624821879\n",
      "Gradient Descent(92/999): loss=10.423871684650003, w0=-1.0544720461940271, w1=0.9075933528421923\n",
      "Gradient Descent(93/999): loss=10.413292038907477, w0=-1.0647155090496532, w1=0.9086224197950221\n",
      "Gradient Descent(94/999): loss=10.402788538957822, w0=-1.0749220422586008, w1=0.909647776764274\n",
      "Gradient Descent(95/999): loss=10.392360636750373, w0=-1.0850917789593233, w1=0.9106694371251497\n",
      "Gradient Descent(96/999): loss=10.382007788179017, w0=-1.095224851810285, w1=0.9116874142046307\n",
      "Gradient Descent(97/999): loss=10.37172945305377, w0=-1.1053213929916903, w1=0.9127017212816524\n",
      "Gradient Descent(98/999): loss=10.36152509507261, w0=-1.115381534207209, w1=0.913712371587277\n",
      "Gradient Descent(99/999): loss=10.351394181793479, w0=-1.1254054066856942, w1=0.914719378304866\n",
      "Gradient Descent(100/999): loss=10.34133618460652, w0=-1.1353931411828935, w1=0.915722754570253\n",
      "Gradient Descent(101/999): loss=10.331350578706479, w0=-1.1453448679831548, w1=0.9167225134719134\n",
      "Gradient Descent(102/999): loss=10.321436843065332, w0=-1.1552607169011262, w1=0.9177186680511371\n",
      "Gradient Descent(103/999): loss=10.31159446040509, w0=-1.1651408172834492, w1=0.9187112313021967\n",
      "Gradient Descent(104/999): loss=10.301822917170815, w0=-1.1749852980104456, w1=0.9197002161725193\n",
      "Gradient Descent(105/999): loss=10.292121703503827, w0=-1.1847942874977992, w1=0.9206856355628522\n",
      "Gradient Descent(106/999): loss=10.282490313215089, w0=-1.1945679136982306, w1=0.921667502327435\n",
      "Gradient Descent(107/999): loss=10.272928243758807, w0=-1.2043063041031663, w1=0.9226458292741638\n",
      "Gradient Descent(108/999): loss=10.263434996206197, w0=-1.2140095857444015, w1=0.9236206291647606\n",
      "Gradient Descent(109/999): loss=10.25401007521946, w0=-1.223677885195758, w1=0.9245919147149386\n",
      "Gradient Descent(110/999): loss=10.244652989025944, w0=-1.2333113285747341, w1=0.9255596985945684\n",
      "Gradient Descent(111/999): loss=10.235363249392455, w0=-1.2429100415441507, w1=0.9265239934278438\n",
      "Gradient Descent(112/999): loss=10.226140371599822, w0=-1.2524741493137903, w1=0.9274848117934449\n",
      "Gradient Descent(113/999): loss=10.21698387441757, w0=-1.2620037766420298, w1=0.9284421662247044\n",
      "Gradient Descent(114/999): loss=10.207893280078833, w0=-1.2714990478374681, w1=0.9293960692097694\n",
      "Gradient Descent(115/999): loss=10.19886811425542, w0=-1.2809600867605484, w1=0.9303465331917646\n",
      "Gradient Descent(116/999): loss=10.189907906033053, w0=-1.2903870168251728, w1=0.9312935705689555\n",
      "Gradient Descent(117/999): loss=10.181012187886813, w0=-1.2997799610003127, w1=0.9322371936949088\n",
      "Gradient Descent(118/999): loss=10.17218049565674, w0=-1.309139041811613, w1=0.9331774148786547\n",
      "Gradient Descent(119/999): loss=10.163412368523604, w0=-1.3184643813429897, w1=0.9341142463848466\n",
      "Gradient Descent(120/999): loss=10.154707348984875, w0=-1.3277561012382235, w1=0.9350477004339222\n",
      "Gradient Descent(121/999): loss=10.146064982830847, w0=-1.3370143227025455, w1=0.9359777892022608\n",
      "Gradient Descent(122/999): loss=10.137484819120935, w0=-1.3462391665042188, w1=0.9369045248223453\n",
      "Gradient Descent(123/999): loss=10.128966410160139, w0=-1.355430752976114, w1=0.9378279193829173\n",
      "Gradient Descent(124/999): loss=10.120509311475708, w0=-1.3645892020172785, w1=0.9387479849291366\n",
      "Gradient Descent(125/999): loss=10.112113081793918, w0=-1.3737146330945005, w1=0.939664733462738\n",
      "Gradient Descent(126/999): loss=10.103777283017072, w0=-1.3828071652438676, w1=0.9405781769421878\n",
      "Gradient Descent(127/999): loss=10.095501480200632, w0=-1.3918669170723197, w1=0.9414883272828394\n",
      "Gradient Descent(128/999): loss=10.08728524153051, w0=-1.4008940067591957, w1=0.9423951963570893\n",
      "Gradient Descent(129/999): loss=10.07912813830057, w0=-1.4098885520577755, w1=0.9432987959945319\n",
      "Gradient Descent(130/999): loss=10.071029744890227, w0=-1.4188506702968158, w1=0.9441991379821133\n",
      "Gradient Descent(131/999): loss=10.062989638742248, w0=-1.4277804783820807, w1=0.9450962340642857\n",
      "Gradient Descent(132/999): loss=10.055007400340724, w0=-1.4366780927978664, w1=0.9459900959431597\n",
      "Gradient Descent(133/999): loss=10.047082613189152, w0=-1.4455436296085211, w1=0.9468807352786587\n",
      "Gradient Descent(134/999): loss=10.039214863788711, w0=-1.4543772044599588, w1=0.9477681636886686\n",
      "Gradient Descent(135/999): loss=10.031403741616698, w0=-1.4631789325811677, w1=0.9486523927491913\n",
      "Gradient Descent(136/999): loss=10.023648839105103, w0=-1.4719489287857135, w1=0.9495334339944949\n",
      "Gradient Descent(137/999): loss=10.015949751619333, w0=-1.480687307473237, w1=0.9504112989172641\n",
      "Gradient Descent(138/999): loss=10.008306077437112, w0=-1.4893941826309463, w1=0.9512859989687503\n",
      "Gradient Descent(139/999): loss=10.000717417727506, w0=-1.4980696678351038, w1=0.9521575455589213\n",
      "Gradient Descent(140/999): loss=9.993183376530133, w0=-1.5067138762525076, w1=0.9530259500566092\n",
      "Gradient Descent(141/999): loss=9.98570356073448, w0=-1.515326920641968, w1=0.9538912237896596\n",
      "Gradient Descent(142/999): loss=9.97827758005941, w0=-1.523908913355778, w1=0.9547533780450791\n",
      "Gradient Descent(143/999): loss=9.970905047032783, w0=-1.5324599663411793, w1=0.9556124240691822\n",
      "Gradient Descent(144/999): loss=9.963585576971251, w0=-1.5409801911418222, w1=0.9564683730677387\n",
      "Gradient Descent(145/999): loss=9.956318787960175, w0=-1.5494696988992205, w1=0.957321236206119\n",
      "Gradient Descent(146/999): loss=9.94910430083371, w0=-1.557928600354202, w1=0.9581710246094405\n",
      "Gradient Descent(147/999): loss=9.941941739155011, w0=-1.5663570058483525, w1=0.9590177493627126\n",
      "Gradient Descent(148/999): loss=9.934830729196591, w0=-1.5747550253254547, w1=0.9598614215109803\n",
      "Gradient Descent(149/999): loss=9.927770899920827, w0=-1.5831227683329239, w1=0.9607020520594702\n",
      "Gradient Descent(150/999): loss=9.920761882960594, w0=-1.5914603440232349, w1=0.961539651973732\n",
      "Gradient Descent(151/999): loss=9.913803312600054, w0=-1.5997678611553474, w1=0.9623742321797831\n",
      "Gradient Descent(152/999): loss=9.906894825755563, w0=-1.6080454280961245, w1=0.9632058035642502\n",
      "Gradient Descent(153/999): loss=9.90003606195673, w0=-1.6162931528217455, w1=0.964034376974512\n",
      "Gradient Descent(154/999): loss=9.893226663327603, w0=-1.6245111429191152, w1=0.9648599632188396\n",
      "Gradient Descent(155/999): loss=9.886466274568008, w0=-1.6326995055872668, w1=0.965682573066539\n",
      "Gradient Descent(156/999): loss=9.879754542935007, w0=-1.6408583476387606, w1=0.9665022172480904\n",
      "Gradient Descent(157/999): loss=9.87309111822448, w0=-1.648987775501077, w1=0.9673189064552885\n",
      "Gradient Descent(158/999): loss=9.866475652752866, w0=-1.6570878952180048, w1=0.9681326513413822\n",
      "Gradient Descent(159/999): loss=9.859907801339018, w0=-1.665158812451025, w1=0.9689434625212133\n",
      "Gradient Descent(160/999): loss=9.853387221286187, w0=-1.6732006324806887, w1=0.9697513505713552\n",
      "Gradient Descent(161/999): loss=9.846913572364146, w0=-1.68121346020799, w1=0.9705563260302505\n",
      "Gradient Descent(162/999): loss=9.840486516791442, w0=-1.689197400155735, w1=0.9713583993983489\n",
      "Gradient Descent(163/999): loss=9.834105719217755, w0=-1.6971525564699055, w1=0.9721575811382439\n",
      "Gradient Descent(164/999): loss=9.827770846706418, w0=-1.7050790329210164, w1=0.9729538816748093\n",
      "Gradient Descent(165/999): loss=9.821481568717036, w0=-1.7129769329054705, w1=0.9737473113953352\n",
      "Gradient Descent(166/999): loss=9.81523755708824, w0=-1.7208463594469063, w1=0.9745378806496638\n",
      "Gradient Descent(167/999): loss=9.809038486020567, w0=-1.7286874151975429, w1=0.9753255997503236\n",
      "Gradient Descent(168/999): loss=9.802884032059456, w0=-1.736500202439518, w1=0.9761104789726652\n",
      "Gradient Descent(169/999): loss=9.796773874078372, w0=-1.744284823086223, w1=0.9768925285549939\n",
      "Gradient Descent(170/999): loss=9.790707693262059, w0=-1.7520413786836313, w1=0.977671758698704\n",
      "Gradient Descent(171/999): loss=9.784685173089885, w0=-1.7597699704116245, w1=0.9784481795684128\n",
      "Gradient Descent(172/999): loss=9.778705999319355, w0=-1.7674706990853108, w1=0.9792218012920906\n",
      "Gradient Descent(173/999): loss=9.772769859969687, w0=-1.7751436651563408, w1=0.9799926339611958\n",
      "Gradient Descent(174/999): loss=9.766876445305542, w0=-1.7827889687142175, w1=0.9807606876308045\n",
      "Gradient Descent(175/999): loss=9.761025447820879, w0=-1.7904067094876022, w1=0.9815259723197424\n",
      "Gradient Descent(176/999): loss=9.755216562222888, w0=-1.797996986845615, w1=0.9822884980107156\n",
      "Gradient Descent(177/999): loss=9.749449485416068, w0=-1.8055598997991316, w1=0.9830482746504403\n",
      "Gradient Descent(178/999): loss=9.743723916486417, w0=-1.8130955470020744, w1=0.9838053121497734\n",
      "Gradient Descent(179/999): loss=9.738039556685726, w0=-1.8206040267526995, w1=0.9845596203838406\n",
      "Gradient Descent(180/999): loss=9.732396109415994, w0=-1.828085436994879, w1=0.9853112091921665\n",
      "Gradient Descent(181/999): loss=9.726793280213945, w0=-1.8355398753193783, w1=0.9860600883788019\n",
      "Gradient Descent(182/999): loss=9.721230776735675, w0=-1.8429674389651298, w1=0.9868062677124522\n",
      "Gradient Descent(183/999): loss=9.71570830874139, w0=-1.8503682248205007, w1=0.9875497569266051\n",
      "Gradient Descent(184/999): loss=9.71022558808026, w0=-1.857742329424557, w1=0.9882905657196567\n",
      "Gradient Descent(185/999): loss=9.70478232867539, w0=-1.8650898489683232, w1=0.9890287037550388\n",
      "Gradient Descent(186/999): loss=9.699378246508894, w0=-1.8724108792960361, w1=0.9897641806613446\n",
      "Gradient Descent(187/999): loss=9.694013059607073, w0=-1.8797055159063965, w1=0.9904970060324546\n",
      "Gradient Descent(188/999): loss=9.68868648802569, w0=-1.8869738539538132, w1=0.9912271894276611\n",
      "Gradient Descent(189/999): loss=9.683398253835389, w0=-1.8942159882496457, w1=0.9919547403717937\n",
      "Gradient Descent(190/999): loss=9.678148081107173, w0=-1.9014320132634401, w1=0.9926796683553427\n",
      "Gradient Descent(191/999): loss=9.672935695898005, w0=-1.908622023124162, w1=0.9934019828345843\n",
      "Gradient Descent(192/999): loss=9.667760826236536, w0=-1.9157861116214236, w1=0.9941216932317017\n",
      "Gradient Descent(193/999): loss=9.662623202108897, w0=-1.9229243722067082, w1=0.9948388089349103\n",
      "Gradient Descent(194/999): loss=9.657522555444597, w0=-1.9300368979945879, w1=0.9955533392985791\n",
      "Gradient Descent(195/999): loss=9.652458620102573, w0=-1.9371237817639393, w1=0.9962652936433523\n",
      "Gradient Descent(196/999): loss=9.647431131857266, w0=-1.9441851159591532, w1=0.996974681256272\n",
      "Gradient Descent(197/999): loss=9.642439828384862, w0=-1.9512209926913409, w1=0.9976815113908979\n",
      "Gradient Descent(198/999): loss=9.637484449249586, w0=-1.9582315037395353, w1=0.9983857932674298\n",
      "Gradient Descent(199/999): loss=9.632564735890119, w0=-1.965216740551888, w1=0.9990875360728265\n",
      "Gradient Descent(200/999): loss=9.627680431606109, w0=-1.9721767942468635, w1=0.9997867489609259\n",
      "Gradient Descent(201/999): loss=9.622831281544782, w0=-1.9791117556144258, w1=1.0004834410525645\n",
      "Gradient Descent(202/999): loss=9.618017032687629, w0=-1.9860217151172237, w1=1.0011776214356973\n",
      "Gradient Descent(203/999): loss=9.613237433837215, w0=-1.9929067628917714, w1=1.001869299165514\n",
      "Gradient Descent(204/999): loss=9.608492235604071, w0=-1.9997669887496234, w1=1.0025584832645598\n",
      "Gradient Descent(205/999): loss=9.603781190393683, w0=-2.0066024821785464, w1=1.0032451827228517\n",
      "Gradient Descent(206/999): loss=9.59910405239356, w0=-2.0134133323436862, w1=1.0039294064979956\n",
      "Gradient Descent(207/999): loss=9.594460577560435, w0=-2.0201996280887315, w1=1.0046111635153032\n",
      "Gradient Descent(208/999): loss=9.589850523607499, w0=-2.0269614579370727, w1=1.0052904626679104\n",
      "Gradient Descent(209/999): loss=9.585273649991786, w0=-2.033698910092956, w1=1.0059673128168891\n",
      "Gradient Descent(210/999): loss=9.580729717901598, w0=-2.040412072442634, w1=1.006641722791368\n",
      "Gradient Descent(211/999): loss=9.576218490244068, w0=-2.0471010325555135, w1=1.0073137013886428\n",
      "Gradient Descent(212/999): loss=9.571739731632777, w0=-2.0537658776852963, w1=1.007983257374295\n",
      "Gradient Descent(213/999): loss=9.567293208375467, w0=-2.060406694771118, w1=1.0086503994823033\n",
      "Gradient Descent(214/999): loss=9.56287868846186, w0=-2.067023570438682, w1=1.0093151364151594\n",
      "Gradient Descent(215/999): loss=9.558495941551536, w0=-2.0736165910013886, w1=1.0099774768439806\n",
      "Gradient Descent(216/999): loss=9.554144738961934, w0=-2.0801858424614634, w1=1.0106374294086233\n",
      "Gradient Descent(217/999): loss=9.549824853656403, w0=-2.086731410511076, w1=1.0112950027177954\n",
      "Gradient Descent(218/999): loss=9.545536060232369, w0=-2.09325338053346, w1=1.011950205349169\n",
      "Gradient Descent(219/999): loss=9.54127813490956, w0=-2.099751837604026, w1=1.0126030458494915\n",
      "Gradient Descent(220/999): loss=9.537050855518341, w0=-2.1062268664914714, w1=1.0132535327346985\n",
      "Gradient Descent(221/999): loss=9.532854001488117, w0=-2.112678551658886, w1=1.013901674490023\n",
      "Gradient Descent(222/999): loss=9.528687353835823, w0=-2.1191069772648543, w1=1.0145474795701084\n",
      "Gradient Descent(223/999): loss=9.524550695154504, w0=-2.1255122271645526, w1=1.0151909563991162\n",
      "Gradient Descent(224/999): loss=9.520443809601959, w0=-2.131894384910844, w1=1.0158321133708377\n",
      "Gradient Descent(225/999): loss=9.516366482889499, w0=-2.1382535337553668, w1=1.0164709588488032\n",
      "Gradient Descent(226/999): loss=9.512318502270734, w0=-2.144589756649621, w1=1.0171075011663908\n",
      "Gradient Descent(227/999): loss=9.508299656530513, w0=-2.150903136246051, w1=1.0177417486269345\n",
      "Gradient Descent(228/999): loss=9.504309735973873, w0=-2.157193754899123, w1=1.0183737095038345\n",
      "Gradient Descent(229/999): loss=9.500348532415103, w0=-2.1634616946664007, w1=1.0190033920406623\n",
      "Gradient Descent(230/999): loss=9.496415839166886, w0=-2.1697070373096126, w1=1.0196308044512712\n",
      "Gradient Descent(231/999): loss=9.492511451029522, w0=-2.175929864295722, w1=1.0202559549199004\n",
      "Gradient Descent(232/999): loss=9.488635164280199, w0=-2.182130256797988, w1=1.0208788516012852\n",
      "Gradient Descent(233/999): loss=9.484786776662386, w0=-2.188308295697024, w1=1.0214995026207598\n",
      "Gradient Descent(234/999): loss=9.480966087375272, w0=-2.194464061581853, w1=1.022117916074366\n",
      "Gradient Descent(235/999): loss=9.477172897063275, w0=-2.2005976347509604, w1=1.0227341000289578\n",
      "Gradient Descent(236/999): loss=9.473407007805664, w0=-2.206709095213339, w1=1.0233480625223061\n",
      "Gradient Descent(237/999): loss=9.469668223106215, w0=-2.2127985226895346, w1=1.0239598115632045\n",
      "Gradient Descent(238/999): loss=9.465956347882967, w0=-2.218865996612685, w1=1.024569355131573\n",
      "Gradient Descent(239/999): loss=9.46227118845803, w0=-2.2249115961295556, w1=1.0251767011785635\n",
      "Gradient Descent(240/999): loss=9.458612552547503, w0=-2.2309354001015733, w1=1.0257818576266609\n",
      "Gradient Descent(241/999): loss=9.454980249251408, w0=-2.2369374871058545, w1=1.0263848323697893\n",
      "Gradient Descent(242/999): loss=9.451374089043764, w0=-2.2429179354362296, w1=1.026985633273413\n",
      "Gradient Descent(243/999): loss=9.447793883762674, w0=-2.2488768231042653, w1=1.0275842681746399\n",
      "Gradient Descent(244/999): loss=9.444239446600518, w0=-2.2548142278402805, w1=1.0281807448823244\n",
      "Gradient Descent(245/999): loss=9.440710592094195, w0=-2.260730227094363, w1=1.0287750711771668\n",
      "Gradient Descent(246/999): loss=9.437207136115466, w0=-2.2666248980373767, w1=1.0293672548118185\n",
      "Gradient Descent(247/999): loss=9.433728895861323, w0=-2.2724983175619706, w1=1.0299573035109792\n",
      "Gradient Descent(248/999): loss=9.430275689844464, w0=-2.278350562283581, w1=1.0305452249715015\n",
      "Gradient Descent(249/999): loss=9.426847337883832, w0=-2.2841817085414307, w1=1.0311310268624874\n",
      "Gradient Descent(250/999): loss=9.423443661095181, w0=-2.2899918323995245, w1=1.0317147168253917\n",
      "Gradient Descent(251/999): loss=9.42006448188179, w0=-2.295781009647643, w1=1.03229630247412\n",
      "Gradient Descent(252/999): loss=9.416709623925149, w0=-2.301549315802329, w1=1.032875791395128\n",
      "Gradient Descent(253/999): loss=9.413378912175785, w0=-2.3072968261078737, w1=1.033453191147521\n",
      "Gradient Descent(254/999): loss=9.410072172844133, w0=-2.313023615537299, w1=1.0340285092631523\n",
      "Gradient Descent(255/999): loss=9.406789233391452, w0=-2.3187297587933346, w1=1.0346017532467213\n",
      "Gradient Descent(256/999): loss=9.403529922520825, w0=-2.324415330309392, w1=1.0351729305758715\n",
      "Gradient Descent(257/999): loss=9.400294070168234, w0=-2.3300804042505363, w1=1.035742048701288\n",
      "Gradient Descent(258/999): loss=9.397081507493668, w0=-2.335725054514453, w1=1.0363091150467953\n",
      "Gradient Descent(259/999): loss=9.393892066872333, w0=-2.341349354732413, w1=1.036874137009453\n",
      "Gradient Descent(260/999): loss=9.390725581885885, w0=-2.3469533782702316, w1=1.037437121959653\n",
      "Gradient Descent(261/999): loss=9.38758188731376, w0=-2.3525371982292267, w1=1.0379980772412156\n",
      "Gradient Descent(262/999): loss=9.384460819124554, w0=-2.3581008874471716, w1=1.0385570101714854\n",
      "Gradient Descent(263/999): loss=9.381362214467464, w0=-2.363644518499246, w1=1.0391139280414259\n",
      "Gradient Descent(264/999): loss=9.37828591166377, w0=-2.3691681636989816, w1=1.0396688381157162\n",
      "Gradient Descent(265/999): loss=9.375231750198438, w0=-2.3746718950992065, w1=1.0402217476328444\n",
      "Gradient Descent(266/999): loss=9.372199570711702, w0=-2.380155784492984, w1=1.0407726638052022\n",
      "Gradient Descent(267/999): loss=9.36918921499079, w0=-2.3856199034145504, w1=1.0413215938191795\n",
      "Gradient Descent(268/999): loss=9.366200525961629, w0=-2.391064323140246, w1=1.0418685448352583\n",
      "Gradient Descent(269/999): loss=9.363233347680683, w0=-2.396489114689448, w1=1.042413523988105\n",
      "Gradient Descent(270/999): loss=9.360287525326795, w0=-2.401894348825494, w1=1.0429565383866641\n",
      "Gradient Descent(271/999): loss=9.357362905193119, w0=-2.4072800960566063, w1=1.043497595114252\n",
      "Gradient Descent(272/999): loss=9.35445933467909, w0=-2.4126464266368117, w1=1.0440367012286476\n",
      "Gradient Descent(273/999): loss=9.351576662282477, w0=-2.417993410566858, w1=1.0445738637621846\n",
      "Gradient Descent(274/999): loss=9.348714737591465, w0=-2.4233211175951266, w1=1.0451090897218454\n",
      "Gradient Descent(275/999): loss=9.345873411276807, w0=-2.4286296172185424, w1=1.0456423860893493\n",
      "Gradient Descent(276/999): loss=9.343052535084041, w0=-2.433918978683481, w1=1.0461737598212462\n",
      "Gradient Descent(277/999): loss=9.340251961825752, w0=-2.4391892709866716, w1=1.0467032178490057\n",
      "Gradient Descent(278/999): loss=9.337471545373882, w0=-2.4444405628760966, w1=1.0472307670791083\n",
      "Gradient Descent(279/999): loss=9.334711140652121, w0=-2.4496729228518888, w1=1.0477564143931355\n",
      "Gradient Descent(280/999): loss=9.33197060362832, w0=-2.4548864191672255, w1=1.0482801666478594\n",
      "Gradient Descent(281/999): loss=9.329249791306998, w0=-2.4600811198292174, w1=1.0488020306753316\n",
      "Gradient Descent(282/999): loss=9.326548561721857, w0=-2.4652570925997965, w1=1.0493220132829733\n",
      "Gradient Descent(283/999): loss=9.32386677392838, w0=-2.470414404996601, w1=1.0498401212536639\n",
      "Gradient Descent(284/999): loss=9.3212042879965, w0=-2.475553124293854, w1=1.0503563613458284\n",
      "Gradient Descent(285/999): loss=9.318560965003266, w0=-2.4806733175232427, w1=1.0508707402935271\n",
      "Gradient Descent(286/999): loss=9.315936667025618, w0=-2.4857750514747923, w1=1.0513832648065424\n",
      "Gradient Descent(287/999): loss=9.313331257133173, w0=-2.4908583926977372, w1=1.0518939415704667\n",
      "Gradient Descent(288/999): loss=9.310744599381097, w0=-2.4959234075013885, w1=1.0524027772467899\n",
      "Gradient Descent(289/999): loss=9.308176558802998, w0=-2.500970161956, w1=1.0529097784729848\n",
      "Gradient Descent(290/999): loss=9.305627001403892, w0=-2.5059987218936293, w1=1.0534149518625968\n",
      "Gradient Descent(291/999): loss=9.303095794153203, w0=-2.511009152908997, w1=1.0539183040053262\n",
      "Gradient Descent(292/999): loss=9.30058280497784, w0=-2.5160015203603425, w1=1.0544198414671173\n",
      "Gradient Descent(293/999): loss=9.298087902755277, w0=-2.5209758893702756, w1=1.054919570790243\n",
      "Gradient Descent(294/999): loss=9.295610957306739, w0=-2.5259323248266266, w1=1.055417498493389\n",
      "Gradient Descent(295/999): loss=9.293151839390394, w0=-2.530870891383293, w1=1.0559136310717414\n",
      "Gradient Descent(296/999): loss=9.290710420694614, w0=-2.535791653461083, w1=1.0564079749970683\n",
      "Gradient Descent(297/999): loss=9.28828657383128, w0=-2.540694675248555, w1=1.0569005367178073\n",
      "Gradient Descent(298/999): loss=9.285880172329131, w0=-2.5455800207028556, w1=1.0573913226591471\n",
      "Gradient Descent(299/999): loss=9.28349109062717, w0=-2.5504477535505528, w1=1.0578803392231129\n",
      "Gradient Descent(300/999): loss=9.281119204068114, w0=-2.555297937288469, w1=1.0583675927886493\n",
      "Gradient Descent(301/999): loss=9.278764388891883, w0=-2.560130635184508, w1=1.0588530897117032\n",
      "Gradient Descent(302/999): loss=9.276426522229148, w0=-2.5649459102784813, w1=1.0593368363253077\n",
      "Gradient Descent(303/999): loss=9.274105482094916, w0=-2.5697438253829286, w1=1.0598188389396639\n",
      "Gradient Descent(304/999): loss=9.271801147382172, w0=-2.5745244430839396, w1=1.0602991038422227\n",
      "Gradient Descent(305/999): loss=9.269513397855548, w0=-2.5792878257419685, w1=1.0607776372977689\n",
      "Gradient Descent(306/999): loss=9.26724211414506, w0=-2.584034035492648, w1=1.0612544455485002\n",
      "Gradient Descent(307/999): loss=9.264987177739874, w0=-2.5887631342476003, w1=1.061729534814111\n",
      "Gradient Descent(308/999): loss=9.26274847098213, w0=-2.593475183695244, w1=1.0622029112918718\n",
      "Gradient Descent(309/999): loss=9.260525877060788, w0=-2.5981702453015996, w1=1.0626745811567113\n",
      "Gradient Descent(310/999): loss=9.258319280005551, w0=-2.6028483803110904, w1=1.063144550561296\n",
      "Gradient Descent(311/999): loss=9.256128564680798, w0=-2.607509649747342, w1=1.063612825636111\n",
      "Gradient Descent(312/999): loss=9.253953616779587, w0=-2.612154114413978, w1=1.0640794124895396\n",
      "Gradient Descent(313/999): loss=9.251794322817684, w0=-2.6167818348954137, w1=1.064544317207944\n",
      "Gradient Descent(314/999): loss=9.249650570127654, w0=-2.6213928715576453, w1=1.0650075458557429\n",
      "Gradient Descent(315/999): loss=9.24752224685296, w0=-2.6259872845490384, w1=1.0654691044754923\n",
      "Gradient Descent(316/999): loss=9.245409241942149, w0=-2.6305651338011122, w1=1.0659289990879628\n",
      "Gradient Descent(317/999): loss=9.243311445143043, w0=-2.635126479029321, w1=1.0663872356922202\n",
      "Gradient Descent(318/999): loss=9.241228746996994, w0=-2.6396713797338345, w1=1.0668438202657013\n",
      "Gradient Descent(319/999): loss=9.239161038833172, w0=-2.644199895200311, w1=1.0672987587642944\n",
      "Gradient Descent(320/999): loss=9.23710821276289, w0=-2.648712084500675, w1=1.0677520571224142\n",
      "Gradient Descent(321/999): loss=9.235070161673972, w0=-2.6532080064938834, w1=1.068203721253082\n",
      "Gradient Descent(322/999): loss=9.23304677922519, w0=-2.6576877198266957, w1=1.0686537570480001\n",
      "Gradient Descent(323/999): loss=9.231037959840663, w0=-2.6621512829344396, w1=1.069102170377632\n",
      "Gradient Descent(324/999): loss=9.22904359870441, w0=-2.666598754041771, w1=1.069548967091275\n",
      "Gradient Descent(325/999): loss=9.227063591754836, w0=-2.6710301911634353, w1=1.0699941530171406\n",
      "Gradient Descent(326/999): loss=9.225097835679318, w0=-2.675445652105024, w1=1.0704377339624263\n",
      "Gradient Descent(327/999): loss=9.22314622790882, w0=-2.679845194463728, w1=1.0708797157133956\n",
      "Gradient Descent(328/999): loss=9.22120866661253, w0=-2.684228875629089, w1=1.07132010403545\n",
      "Gradient Descent(329/999): loss=9.219285050692552, w0=-2.6885967527837487, w1=1.0717589046732061\n",
      "Gradient Descent(330/999): loss=9.21737527977863, w0=-2.6929488829041945, w1=1.0721961233505706\n",
      "Gradient Descent(331/999): loss=9.215479254222918, w0=-2.6972853227615023, w1=1.072631765770813\n",
      "Gradient Descent(332/999): loss=9.213596875094764, w0=-2.7016061289220783, w1=1.0730658376166429\n",
      "Gradient Descent(333/999): loss=9.211728044175567, w0=-2.7059113577483944, w1=1.0734983445502821\n",
      "Gradient Descent(334/999): loss=9.209872663953636, w0=-2.7102010653997266, w1=1.0739292922135386\n",
      "Gradient Descent(335/999): loss=9.208030637619112, w0=-2.7144753078328847, w1=1.0743586862278813\n",
      "Gradient Descent(336/999): loss=9.206201869058917, w0=-2.7187341408029444, w1=1.0747865321945118\n",
      "Gradient Descent(337/999): loss=9.204386262851731, w0=-2.7229776198639732, w1=1.0752128356944395\n",
      "Gradient Descent(338/999): loss=9.202583724263024, w0=-2.727205800369756, w1=1.0756376022885517\n",
      "Gradient Descent(339/999): loss=9.200794159240099, w0=-2.7314187374745154, w1=1.076060837517689\n",
      "Gradient Descent(340/999): loss=9.199017474407198, w0=-2.735616486133634, w1=1.076482546902715\n",
      "Gradient Descent(341/999): loss=9.197253577060625, w0=-2.739799101104369, w1=1.0769027359445908\n",
      "Gradient Descent(342/999): loss=9.195502375163903, w0=-2.7439666369465674, w1=1.0773214101244444\n",
      "Gradient Descent(343/999): loss=9.193763777342983, w0=-2.748119148023377, w1=1.0777385749036439\n",
      "Gradient Descent(344/999): loss=9.19203769288147, w0=-2.752256688501957, w1=1.0781542357238678\n",
      "Gradient Descent(345/999): loss=9.190324031715885, w0=-2.756379312354182, w1=1.0785683980071765\n",
      "Gradient Descent(346/999): loss=9.188622704430973, w0=-2.76048707335735, w1=1.0789810671560827\n",
      "Gradient Descent(347/999): loss=9.186933622255037, w0=-2.764580025094879, w1=1.079392248553622\n",
      "Gradient Descent(348/999): loss=9.185256697055301, w0=-2.7686582209570108, w1=1.0798019475634235\n",
      "Gradient Descent(349/999): loss=9.183591841333314, w0=-2.772721714141503, w1=1.0802101695297788\n",
      "Gradient Descent(350/999): loss=9.181938968220393, w0=-2.776770557654327, w1=1.0806169197777125\n",
      "Gradient Descent(351/999): loss=9.180297991473072, w0=-2.7808048043103564, w1=1.081022203613052\n",
      "Gradient Descent(352/999): loss=9.178668825468622, w0=-2.784824506734057, w1=1.081426026322495\n",
      "Gradient Descent(353/999): loss=9.177051385200562, w0=-2.7888297173601737, w1=1.0818283931736814\n",
      "Gradient Descent(354/999): loss=9.175445586274252, w0=-2.7928204884344145, w1=1.082229309415258\n",
      "Gradient Descent(355/999): loss=9.173851344902454, w0=-2.7967968720141307, w1=1.0826287802769519\n",
      "Gradient Descent(356/999): loss=9.172268577900992, w0=-2.8007589199689975, w1=1.0830268109696328\n",
      "Gradient Descent(357/999): loss=9.170697202684396, w0=-2.80470668398169, w1=1.0834234066853876\n",
      "Gradient Descent(358/999): loss=9.169137137261592, w0=-2.808640215548557, w1=1.0838185725975822\n",
      "Gradient Descent(359/999): loss=9.167588300231637, w0=-2.8125595659802927, w1=1.084212313860933\n",
      "Gradient Descent(360/999): loss=9.166050610779445, w0=-2.816464786402608, w1=1.0846046356115715\n",
      "Gradient Descent(361/999): loss=9.1645239886716, w0=-2.8203559277568937, w1=1.0849955429671134\n",
      "Gradient Descent(362/999): loss=9.163008354252161, w0=-2.824233040800889, w1=1.0853850410267243\n",
      "Gradient Descent(363/999): loss=9.161503628438483, w0=-2.8280961761093404, w1=1.0857731348711857\n",
      "Gradient Descent(364/999): loss=9.160009732717132, w0=-2.831945384074664, w1=1.0861598295629626\n",
      "Gradient Descent(365/999): loss=9.158526589139747, w0=-2.8357807149076, w1=1.0865451301462683\n",
      "Gradient Descent(366/999): loss=9.157054120319005, w0=-2.8396022186378707, w1=1.0869290416471313\n",
      "Gradient Descent(367/999): loss=9.155592249424556, w0=-2.8434099451148307, w1=1.08731156907346\n",
      "Gradient Descent(368/999): loss=9.15414090017904, w0=-2.8472039440081187, w1=1.0876927174151079\n",
      "Gradient Descent(369/999): loss=9.15269999685409, w0=-2.8509842648083046, w1=1.0880724916439404\n",
      "Gradient Descent(370/999): loss=9.151269464266381, w0=-2.854750956827535, w1=1.0884508967138964\n",
      "Gradient Descent(371/999): loss=9.14984922777372, w0=-2.8585040692001775, w1=1.0888279375610574\n",
      "Gradient Descent(372/999): loss=9.148439213271132, w0=-2.8622436508834603, w1=1.0892036191037064\n",
      "Gradient Descent(373/999): loss=9.147039347187018, w0=-2.865969750658112, w1=1.0895779462423985\n",
      "Gradient Descent(374/999): loss=9.145649556479286, w0=-2.8696824171289963, w1=1.089950923860018\n",
      "Gradient Descent(375/999): loss=9.14426976863157, w0=-2.873381698725748, w1=1.0903225568218478\n",
      "Gradient Descent(376/999): loss=9.14289991164942, w0=-2.8770676437034037, w1=1.0906928499756292\n",
      "Gradient Descent(377/999): loss=9.141539914056573, w0=-2.8807403001430307, w1=1.0910618081516277\n",
      "Gradient Descent(378/999): loss=9.140189704891194, w0=-2.8843997159523553, w1=1.0914294361626933\n",
      "Gradient Descent(379/999): loss=9.138849213702194, w0=-2.8880459388663873, w1=1.091795738804326\n",
      "Gradient Descent(380/999): loss=9.137518370545548, w0=-2.8916790164480424, w1=1.0921607208547364\n",
      "Gradient Descent(381/999): loss=9.136197105980646, w0=-2.8952989960887634, w1=1.0925243870749088\n",
      "Gradient Descent(382/999): loss=9.134885351066664, w0=-2.898905925009137, w1=1.0928867422086637\n",
      "Gradient Descent(383/999): loss=9.133583037358978, w0=-2.9024998502595114, w1=1.0932477909827185\n",
      "Gradient Descent(384/999): loss=9.132290096905576, w0=-2.906080818720609, w1=1.0936075381067507\n",
      "Gradient Descent(385/999): loss=9.13100646224354, w0=-2.909648877104138, w1=1.0939659882734578\n",
      "Gradient Descent(386/999): loss=9.12973206639549, w0=-2.9132040719534027, w1=1.0943231461586198\n",
      "Gradient Descent(387/999): loss=9.12846684286612, w0=-2.916746449643909, w1=1.0946790164211588\n",
      "Gradient Descent(388/999): loss=9.127210725638719, w0=-2.9202760563839707, w1=1.0950336037032018\n",
      "Gradient Descent(389/999): loss=9.125963649171707, w0=-2.9237929382153114, w1=1.0953869126301388\n",
      "Gradient Descent(390/999): loss=9.124725548395249, w0=-2.9272971410136654, w1=1.0957389478106851\n",
      "Gradient Descent(391/999): loss=9.123496358707836, w0=-2.930788710489377, w1=1.0960897138369405\n",
      "Gradient Descent(392/999): loss=9.122276015972922, w0=-2.9342676921879947, w1=1.09643921528445\n",
      "Gradient Descent(393/999): loss=9.121064456515574, w0=-2.937734131490868, w1=1.0967874567122613\n",
      "Gradient Descent(394/999): loss=9.119861617119156, w0=-2.941188073615737, w1=1.0971344426629877\n",
      "Gradient Descent(395/999): loss=9.118667435022022, w0=-2.9446295636173234, w1=1.0974801776628647\n",
      "Gradient Descent(396/999): loss=9.11748184791425, w0=-2.948058646387918, w1=1.097824666221809\n",
      "Gradient Descent(397/999): loss=9.11630479393439, w0=-2.951475366657966, w1=1.09816791283348\n",
      "Gradient Descent(398/999): loss=9.115136211666218, w0=-2.9548797689966517, w1=1.0985099219753351\n",
      "Gradient Descent(399/999): loss=9.113976040135562, w0=-2.9582718978124776, w1=1.0988506981086905\n",
      "Gradient Descent(400/999): loss=9.112824218807098, w0=-2.961651797353846, w1=1.0991902456787777\n",
      "Gradient Descent(401/999): loss=9.111680687581195, w0=-2.965019511709635, w1=1.0995285691148031\n",
      "Gradient Descent(402/999): loss=9.110545386790793, w0=-2.968375084809774, w1=1.0998656728300042\n",
      "Gradient Descent(403/999): loss=9.109418257198266, w0=-2.971718560425816, w1=1.1002015612217089\n",
      "Gradient Descent(404/999): loss=9.108299239992352, w0=-2.9750499821715097, w1=1.1005362386713908\n",
      "Gradient Descent(405/999): loss=9.107188276785074, w0=-2.978369393503368, w1=1.1008697095447282\n",
      "Gradient Descent(406/999): loss=9.106085309608693, w0=-2.9816768377212344, w1=1.10120197819166\n",
      "Gradient Descent(407/999): loss=9.104990280912691, w0=-2.984972357968848, w1=1.1015330489464428\n",
      "Gradient Descent(408/999): loss=9.103903133560763, w0=-2.9882559972344067, w1=1.101862926127707\n",
      "Gradient Descent(409/999): loss=9.102823810827836, w0=-2.991527798351128, w1=1.1021916140385146\n",
      "Gradient Descent(410/999): loss=9.101752256397106, w0=-2.994787803997807, w1=1.102519116966413\n",
      "Gradient Descent(411/999): loss=9.100688414357101, w0=-2.998036056699374, w1=1.102845439183493\n",
      "Gradient Descent(412/999): loss=9.099632229198772, w0=-3.0012725988274482, w1=1.1031705849464428\n",
      "Gradient Descent(413/999): loss=9.098583645812589, w0=-3.004497472600891, w1=1.1034945584966058\n",
      "Gradient Descent(414/999): loss=9.097542609485657, w0=-3.0077107200863575, w1=1.103817364060033\n",
      "Gradient Descent(415/999): loss=9.096509065898887, w0=-3.0109123831988436, w1=1.104139005847541\n",
      "Gradient Descent(416/999): loss=9.095482961124134, w0=-3.014102503702234, w1=1.1044594880547645\n",
      "Gradient Descent(417/999): loss=9.094464241621395, w0=-3.017281123209847, w1=1.104778814862213\n",
      "Gradient Descent(418/999): loss=9.093452854236018, w0=-3.020448283184976, w1=1.1050969904353243\n",
      "Gradient Descent(419/999): loss=9.092448746195927, w0=-3.0236040249414318, w1=1.105414018924518\n",
      "Gradient Descent(420/999): loss=9.091451865108864, w0=-3.026748389644081, w1=1.1057299044652524\n",
      "Gradient Descent(421/999): loss=9.09046215895966, w0=-3.0298814183093827, w1=1.1060446511780753\n",
      "Gradient Descent(422/999): loss=9.08947957610752, w0=-3.0330031518059246, w1=1.1063582631686797\n",
      "Gradient Descent(423/999): loss=9.08850406528332, w0=-3.0361136308549543, w1=1.1066707445279562\n",
      "Gradient Descent(424/999): loss=9.087535575586953, w0=-3.0392128960309117, w1=1.106982099332048\n",
      "Gradient Descent(425/999): loss=9.086574056484652, w0=-3.0423009877619585, w1=1.107292331642402\n",
      "Gradient Descent(426/999): loss=9.085619457806356, w0=-3.045377946330505, w1=1.1076014455058236\n",
      "Gradient Descent(427/999): loss=9.084671729743107, w0=-3.048443811873735, w1=1.1079094449545281\n",
      "Gradient Descent(428/999): loss=9.08373082284444, w0=-3.051498624384132, w1=1.1082163340061943\n",
      "Gradient Descent(429/999): loss=9.082796688015797, w0=-3.0545424237099965, w1=1.1085221166640162\n",
      "Gradient Descent(430/999): loss=9.08186927651598, w0=-3.0575752495559696, w1=1.1088267969167558\n",
      "Gradient Descent(431/999): loss=9.08094853995459, w0=-3.0605971414835493, w1=1.1091303787387943\n",
      "Gradient Descent(432/999): loss=9.08003443028953, w0=-3.063608138911607, w1=1.109432866090185\n",
      "Gradient Descent(433/999): loss=9.079126899824463, w0=-3.066608281116901, w1=1.109734262916705\n",
      "Gradient Descent(434/999): loss=9.07822590120635, w0=-3.0695976072345896, w1=1.1100345731499046\n",
      "Gradient Descent(435/999): loss=9.077331387422964, w0=-3.0725761562587417, w1=1.1103338007071613\n",
      "Gradient Descent(436/999): loss=9.076443311800455, w0=-3.075543967042845, w1=1.1106319494917294\n",
      "Gradient Descent(437/999): loss=9.075561628000889, w0=-3.0785010783003126, w1=1.1109290233927909\n",
      "Gradient Descent(438/999): loss=9.074686290019851, w0=-3.0814475286049885, w1=1.1112250262855075\n",
      "Gradient Descent(439/999): loss=9.073817252184035, w0=-3.0843833563916507, w1=1.1115199620310685\n",
      "Gradient Descent(440/999): loss=9.072954469148868, w0=-3.087308599956512, w1=1.1118138344767452\n",
      "Gradient Descent(441/999): loss=9.072097895896137, w0=-3.090223297457721, w1=1.112106647455936\n",
      "Gradient Descent(442/999): loss=9.071247487731634, w0=-3.093127486915858, w1=1.1123984047882218\n",
      "Gradient Descent(443/999): loss=9.070403200282849, w0=-3.0960212062144317, w1=1.1126891102794116\n",
      "Gradient Descent(444/999): loss=9.069564989496625, w0=-3.098904493100374, w1=1.1129787677215939\n",
      "Gradient Descent(445/999): loss=9.068732811636874, w0=-3.101777385184532, w1=1.1132673808931868\n",
      "Gradient Descent(446/999): loss=9.067906623282303, w0=-3.104639919942158, w1=1.113554953558985\n",
      "Gradient Descent(447/999): loss=9.067086381324126, w0=-3.1074921347133992, w1=1.1138414894702122\n",
      "Gradient Descent(448/999): loss=9.066272042963835, w0=-3.110334066703784, w1=1.114126992364567\n",
      "Gradient Descent(449/999): loss=9.06546356571096, w0=-3.1131657529847088, w1=1.1144114659662727\n",
      "Gradient Descent(450/999): loss=9.064660907380844, w0=-3.1159872304939187, w1=1.1146949139861273\n",
      "Gradient Descent(451/999): loss=9.063864026092464, w0=-3.1187985360359924, w1=1.1149773401215497\n",
      "Gradient Descent(452/999): loss=9.063072880266212, w0=-3.1215997062828214, w1=1.1152587480566287\n",
      "Gradient Descent(453/999): loss=9.06228742862176, w0=-3.1243907777740865, w1=1.1155391414621723\n",
      "Gradient Descent(454/999): loss=9.061507630175882, w0=-3.127171786917738, w1=1.1158185239957532\n",
      "Gradient Descent(455/999): loss=9.060733444240325, w0=-3.1299427699904663, w1=1.1160968993017595\n",
      "Gradient Descent(456/999): loss=9.059964830419688, w0=-3.132703763138179, w1=1.1163742710114384\n",
      "Gradient Descent(457/999): loss=9.05920174860931, w0=-3.1354548023764703, w1=1.1166506427429483\n",
      "Gradient Descent(458/999): loss=9.058444158993177, w0=-3.1381959235910912, w1=1.1169260181014011\n",
      "Gradient Descent(459/999): loss=9.057692022041852, w0=-3.140927162538418, w1=1.1172004006789131\n",
      "Gradient Descent(460/999): loss=9.056945298510394, w0=-3.1436485548459174, w1=1.11747379405465\n",
      "Gradient Descent(461/999): loss=9.056203949436341, w0=-3.146360136012614, w1=1.1177462017948734\n",
      "Gradient Descent(462/999): loss=9.055467936137642, w0=-3.14906194140955, w1=1.1180176274529883\n",
      "Gradient Descent(463/999): loss=9.054737220210667, w0=-3.151754006280249, w1=1.118288074569589\n",
      "Gradient Descent(464/999): loss=9.054011763528186, w0=-3.154436365741175, w1=1.1185575466725053\n",
      "Gradient Descent(465/999): loss=9.053291528237386, w0=-3.15710905478219, w1=1.118826047276848\n",
      "Gradient Descent(466/999): loss=9.052576476757897, w0=-3.1597721082670107, w1=1.119093579885056\n",
      "Gradient Descent(467/999): loss=9.05186657177983, w0=-3.1624255609336642, w1=1.1193601479869408\n",
      "Gradient Descent(468/999): loss=9.051161776261825, w0=-3.16506944739494, w1=1.1196257550597326\n",
      "Gradient Descent(469/999): loss=9.05046205342913, w0=-3.1677038021388415, w1=1.1198904045681253\n",
      "Gradient Descent(470/999): loss=9.04976736677167, w0=-3.1703286595290368, w1=1.120154099964322\n",
      "Gradient Descent(471/999): loss=9.049077680042148, w0=-3.1729440538053058, w1=1.1204168446880802\n",
      "Gradient Descent(472/999): loss=9.048392957254155, w0=-3.1755500190839876, w1=1.120678642166756\n",
      "Gradient Descent(473/999): loss=9.047713162680285, w0=-3.178146589358426, w1=1.12093949581535\n",
      "Gradient Descent(474/999): loss=9.047038260850284, w0=-3.1807337984994115, w1=1.1211994090365498\n",
      "Gradient Descent(475/999): loss=9.04636821654918, w0=-3.183311680255624, w1=1.121458385220777\n",
      "Gradient Descent(476/999): loss=9.045702994815468, w0=-3.1858802682540737, w1=1.1217164277462288\n",
      "Gradient Descent(477/999): loss=9.045042560939267, w0=-3.188439596000538, w1=1.121973539978925\n",
      "Gradient Descent(478/999): loss=9.044386880460522, w0=-3.19098969688, w1=1.122229725272748\n",
      "Gradient Descent(479/999): loss=9.043735919167196, w0=-3.1935306041570835, w1=1.1224849869694913\n",
      "Gradient Descent(480/999): loss=9.043089643093486, w0=-3.196062350976487, w1=1.1227393283988985\n",
      "Gradient Descent(481/999): loss=9.042448018518066, w0=-3.198584970363416, w1=1.12299275287871\n",
      "Gradient Descent(482/999): loss=9.041811011962306, w0=-3.201098495224014, w1=1.1232452637147052\n",
      "Gradient Descent(483/999): loss=9.041178590188533, w0=-3.203602958345791, w1=1.1234968642007443\n",
      "Gradient Descent(484/999): loss=9.040550720198302, w0=-3.2060983923980517, w1=1.1237475576188143\n",
      "Gradient Descent(485/999): loss=9.039927369230677, w0=-3.208584829932323, w1=1.1239973472390683\n",
      "Gradient Descent(486/999): loss=9.039308504760498, w0=-3.211062303382776, w1=1.1242462363198713\n",
      "Gradient Descent(487/999): loss=9.03869409449672, w0=-3.2135308450666504, w1=1.1244942281078398\n",
      "Gradient Descent(488/999): loss=9.0380841063807, w0=-3.2159904871846767, w1=1.1247413258378873\n",
      "Gradient Descent(489/999): loss=9.03747850858453, w0=-3.218441261821495, w1=1.1249875327332635\n",
      "Gradient Descent(490/999): loss=9.036877269509386, w0=-3.220883200946075, w1=1.1252328520055979\n",
      "Gradient Descent(491/999): loss=9.036280357783872, w0=-3.223316336412131, w1=1.125477286854942\n",
      "Gradient Descent(492/999): loss=9.03568774226238, w0=-3.2257406999585396, w1=1.1257208404698098\n",
      "Gradient Descent(493/999): loss=9.035099392023476, w0=-3.228156323209752, w1=1.1259635160272214\n",
      "Gradient Descent(494/999): loss=9.03451527636827, w0=-3.2305632376762077, w1=1.1262053166927408\n",
      "Gradient Descent(495/999): loss=9.033935364818836, w0=-3.2329614747547444, w1=1.1264462456205224\n",
      "Gradient Descent(496/999): loss=9.033359627116596, w0=-3.2353510657290085, w1=1.1266863059533467\n",
      "Gradient Descent(497/999): loss=9.032788033220765, w0=-3.237732041769863, w1=1.1269255008226655\n",
      "Gradient Descent(498/999): loss=9.032220553306766, w0=-3.2401044339357936, w1=1.1271638333486405\n",
      "Gradient Descent(499/999): loss=9.031657157764688, w0=-3.2424682731733148, w1=1.1274013066401845\n",
      "Gradient Descent(500/999): loss=9.031097817197727, w0=-3.244823590317372, w1=1.1276379237950027\n",
      "Gradient Descent(501/999): loss=9.030542502420667, w0=-3.247170416091746, w1=1.1278736878996318\n",
      "Gradient Descent(502/999): loss=9.029991184458341, w0=-3.2495087811094514, w1=1.128108602029481\n",
      "Gradient Descent(503/999): loss=9.02944383454413, w0=-3.251838715873138, w1=1.128342669248873\n",
      "Gradient Descent(504/999): loss=9.028900424118461, w0=-3.2541602507754863, w1=1.1285758926110814\n",
      "Gradient Descent(505/999): loss=9.02836092482732, w0=-3.256473416099607, w1=1.128808275158374\n",
      "Gradient Descent(506/999): loss=9.027825308520756, w0=-3.258778242019433, w1=1.1290398199220486\n",
      "Gradient Descent(507/999): loss=9.027293547251432, w0=-3.261074758600115, w1=1.1292705299224768\n",
      "Gradient Descent(508/999): loss=9.026765613273158, w0=-3.2633629957984134, w1=1.1295004081691393\n",
      "Gradient Descent(509/999): loss=9.026241479039447, w0=-3.265642983463088, w1=1.1297294576606682\n",
      "Gradient Descent(510/999): loss=9.025721117202067, w0=-3.267914751335289, w1=1.1299576813848846\n",
      "Gradient Descent(511/999): loss=9.025204500609627, w0=-3.270178329048943, w1=1.1301850823188377\n",
      "Gradient Descent(512/999): loss=9.024691602306152, w0=-3.2724337461311412, w1=1.1304116634288444\n",
      "Gradient Descent(513/999): loss=9.024182395529689, w0=-3.2746810320025244, w1=1.1306374276705267\n",
      "Gradient Descent(514/999): loss=9.023676853710883, w0=-3.276920215977665, w1=1.1308623779888516\n",
      "Gradient Descent(515/999): loss=9.023174950471624, w0=-3.2791513272654527, w1=1.131086517318169\n",
      "Gradient Descent(516/999): loss=9.022676659623656, w0=-3.281374394969472, w1=1.1313098485822497\n",
      "Gradient Descent(517/999): loss=9.022181955167197, w0=-3.2835894480883834, w1=1.1315323746943236\n",
      "Gradient Descent(518/999): loss=9.02169081128961, w0=-3.285796515516303, w1=1.1317540985571177\n",
      "Gradient Descent(519/999): loss=9.02120320236403, w0=-3.2879956260431764, w1=1.1319750230628949\n",
      "Gradient Descent(520/999): loss=9.020719102948044, w0=-3.2901868083551573, w1=1.1321951510934902\n",
      "Gradient Descent(521/999): loss=9.020238487782358, w0=-3.2923700910349796, w1=1.1324144855203493\n",
      "Gradient Descent(522/999): loss=9.01976133178948, w0=-3.294545502562331, w1=1.1326330292045657\n",
      "Gradient Descent(523/999): loss=9.019287610072409, w0=-3.296713071314225, w1=1.1328507849969183\n",
      "Gradient Descent(524/999): loss=9.01881729791333, w0=-3.29887282556537, w1=1.133067755737908\n",
      "Gradient Descent(525/999): loss=9.018350370772348, w0=-3.3010247934885384, w1=1.1332839442577958\n",
      "Gradient Descent(526/999): loss=9.017886804286174, w0=-3.303169003154935, w1=1.1334993533766384\n",
      "Gradient Descent(527/999): loss=9.017426574266882, w0=-3.3053054825345622, w1=1.133713985904326\n",
      "Gradient Descent(528/999): loss=9.016969656700631, w0=-3.3074342594965858, w1=1.1339278446406185\n",
      "Gradient Descent(529/999): loss=9.016516027746418, w0=-3.3095553618096964, w1=1.1341409323751823\n",
      "Gradient Descent(530/999): loss=9.016065663734837, w0=-3.311668817142475, w1=1.1343532518876263\n",
      "Gradient Descent(531/999): loss=9.015618541166832, w0=-3.3137746530637506, w1=1.1345648059475386\n",
      "Gradient Descent(532/999): loss=9.015174636712482, w0=-3.3158728970429623, w1=1.1347755973145217\n",
      "Gradient Descent(533/999): loss=9.014733927209786, w0=-3.317963576450516, w1=1.1349856287382307\n",
      "Gradient Descent(534/999): loss=9.01429638966344, w0=-3.320046718558142, w1=1.135194902958406\n",
      "Gradient Descent(535/999): loss=9.013862001243654, w0=-3.322122350539251, w1=1.1354034227049117\n",
      "Gradient Descent(536/999): loss=9.013430739284942, w0=-3.3241904994692892, w1=1.1356111906977702\n",
      "Gradient Descent(537/999): loss=9.013002581284963, w0=-3.326251192326089, w1=1.135818209647197\n",
      "Gradient Descent(538/999): loss=9.012577504903328, w0=-3.328304455990223, w1=1.1360244822536376\n",
      "Gradient Descent(539/999): loss=9.012155487960436, w0=-3.3303503172453555, w1=1.1362300112078012\n",
      "Gradient Descent(540/999): loss=9.011736508436334, w0=-3.332388802778589, w1=1.1364347991906973\n",
      "Gradient Descent(541/999): loss=9.01132054446954, w0=-3.3344199391808145, w1=1.1366388488736685\n",
      "Gradient Descent(542/999): loss=9.010907574355926, w0=-3.3364437529470576, w1=1.1368421629184284\n",
      "Gradient Descent(543/999): loss=9.010497576547575, w0=-3.3384602704768245, w1=1.137044743977093\n",
      "Gradient Descent(544/999): loss=9.010090529651663, w0=-3.3404695180744457, w1=1.1372465946922181\n",
      "Gradient Descent(545/999): loss=9.009686412429328, w0=-3.3424715219494203, w1=1.1374477176968318\n",
      "Gradient Descent(546/999): loss=9.009285203794585, w0=-3.3444663082167563, w1=1.1376481156144704\n",
      "Gradient Descent(547/999): loss=9.0088868828132, w0=-3.3464539028973124, w1=1.1378477910592109\n",
      "Gradient Descent(548/999): loss=9.008491428701626, w0=-3.348434331918137, w1=1.138046746635707\n",
      "Gradient Descent(549/999): loss=9.00809882082589, w0=-3.3504076211128075, w1=1.138244984939221\n",
      "Gradient Descent(550/999): loss=9.007709038700533, w0=-3.3523737962217646, w1=1.1384425085556606\n",
      "Gradient Descent(551/999): loss=9.007322061987539, w0=-3.354332882892651, w1=1.1386393200616085\n",
      "Gradient Descent(552/999): loss=9.006937870495278, w0=-3.3562849066806444, w1=1.1388354220243604\n",
      "Gradient Descent(553/999): loss=9.006556444177438, w0=-3.358229893048791, w1=1.1390308170019545\n",
      "Gradient Descent(554/999): loss=9.00617776313199, w0=-3.360167867368338, w1=1.139225507543209\n",
      "Gradient Descent(555/999): loss=9.005801807600152, w0=-3.362098854919065, w1=1.1394194961877506\n",
      "Gradient Descent(556/999): loss=9.005428557965347, w0=-3.3640228808896118, w1=1.1396127854660527\n",
      "Gradient Descent(557/999): loss=9.005057994752194, w0=-3.3659399703778097, w1=1.139805377899464\n",
      "Gradient Descent(558/999): loss=9.004690098625481, w0=-3.3678501483910064, w1=1.1399972760002437\n",
      "Gradient Descent(559/999): loss=9.004324850389155, w0=-3.369753439846394, w1=1.1401884822715944\n",
      "Gradient Descent(560/999): loss=9.003962230985321, w0=-3.3716498695713337, w1=1.1403789992076934\n",
      "Gradient Descent(561/999): loss=9.003602221493267, w0=-3.373539462303678, w1=1.1405688292937268\n",
      "Gradient Descent(562/999): loss=9.00324480312845, w0=-3.3754222426920957, w1=1.1407579750059205\n",
      "Gradient Descent(563/999): loss=9.002889957241523, w0=-3.3772982352963923, w1=1.1409464388115735\n",
      "Gradient Descent(564/999): loss=9.002537665317371, w0=-3.3791674645878302, w1=1.141134223169089\n",
      "Gradient Descent(565/999): loss=9.00218790897414, w0=-3.3810299549494482, w1=1.1413213305280085\n",
      "Gradient Descent(566/999): loss=9.001840669962272, w0=-3.38288573067638, w1=1.1415077633290414\n",
      "Gradient Descent(567/999): loss=9.001495930163568, w0=-3.384734815976171, w1=1.141693524004098\n",
      "Gradient Descent(568/999): loss=9.00115367159022, w0=-3.3865772349690926, w1=1.1418786149763214\n",
      "Gradient Descent(569/999): loss=9.000813876383898, w0=-3.3884130116884585, w1=1.1420630386601185\n",
      "Gradient Descent(570/999): loss=9.000476526814793, w0=-3.3902421700809384, w1=1.142246797461192\n",
      "Gradient Descent(571/999): loss=9.000141605280712, w0=-3.3920647340068686, w1=1.142429893776572\n",
      "Gradient Descent(572/999): loss=8.999809094306155, w0=-3.393880727240565, w1=1.1426123299946456\n",
      "Gradient Descent(573/999): loss=8.999478976541386, w0=-3.3956901734706317, w1=1.1427941084951907\n",
      "Gradient Descent(574/999): loss=8.999151234761557, w0=-3.3974930963002725, w1=1.1429752316494048\n",
      "Gradient Descent(575/999): loss=8.998825851865785, w0=-3.3992895192475956, w1=1.143155701819937\n",
      "Gradient Descent(576/999): loss=8.998502810876275, w0=-3.4010794657459225, w1=1.1433355213609186\n",
      "Gradient Descent(577/999): loss=8.998182094937418, w0=-3.4028629591440924, w1=1.1435146926179942\n",
      "Gradient Descent(578/999): loss=8.997863687314931, w0=-3.404640022706769, w1=1.1436932179283508\n",
      "Gradient Descent(579/999): loss=8.99754757139497, w0=-3.406410679614741, w1=1.1438710996207506\n",
      "Gradient Descent(580/999): loss=8.997233730683266, w0=-3.4081749529652265, w1=1.1440483400155599\n",
      "Gradient Descent(581/999): loss=8.99692214880427, w0=-3.4099328657721735, w1=1.1442249414247787\n",
      "Gradient Descent(582/999): loss=8.996612809500292, w0=-3.4116844409665603, w1=1.1444009061520732\n",
      "Gradient Descent(583/999): loss=8.996305696630648, w0=-3.413429701396695, w1=1.1445762364928032\n",
      "Gradient Descent(584/999): loss=8.996000794170838, w0=-3.415168669828513, w1=1.1447509347340543\n",
      "Gradient Descent(585/999): loss=8.995698086211684, w0=-3.4169013689458736, w1=1.144925003154666\n",
      "Gradient Descent(586/999): loss=8.995397556958524, w0=-3.418627821350857, w1=1.1450984440252623\n",
      "Gradient Descent(587/999): loss=8.995099190730365, w0=-3.420348049564059, w1=1.1452712596082817\n",
      "Gradient Descent(588/999): loss=8.994802971959082, w0=-3.4220620760248828, w1=1.1454434521580052\n",
      "Gradient Descent(589/999): loss=8.994508885188601, w0=-3.423769923091835, w1=1.145615023920588\n",
      "Gradient Descent(590/999): loss=8.994216915074087, w0=-3.4254716130428147, w1=1.1457859771340857\n",
      "Gradient Descent(591/999): loss=8.993927046381156, w0=-3.427167168075405, w1=1.1459563140284872\n",
      "Gradient Descent(592/999): loss=8.993639263985061, w0=-3.4288566103071623, w1=1.1461260368257404\n",
      "Gradient Descent(593/999): loss=8.99335355286992, w0=-3.4305399617759047, w1=1.1462951477397834\n",
      "Gradient Descent(594/999): loss=8.99306989812792, w0=-3.4322172444400003, w1=1.1464636489765725\n",
      "Gradient Descent(595/999): loss=8.992788284958559, w0=-3.4338884801786533, w1=1.1466315427341107\n",
      "Gradient Descent(596/999): loss=8.992508698667846, w0=-3.4355536907921884, w1=1.1467988312024777\n",
      "Gradient Descent(597/999): loss=8.992231124667553, w0=-3.437212898002336, w1=1.1469655165638566\n",
      "Gradient Descent(598/999): loss=8.991955548474449, w0=-3.4388661234525166, w1=1.1471316009925634\n",
      "Gradient Descent(599/999): loss=8.991681955709545, w0=-3.440513388708121, w1=1.1472970866550762\n",
      "Gradient Descent(600/999): loss=8.991410332097349, w0=-3.4421547152567924, w1=1.1474619757100613\n",
      "Gradient Descent(601/999): loss=8.991140663465098, w0=-3.4437901245087077, w1=1.1476262703084037\n",
      "Gradient Descent(602/999): loss=8.990872935742054, w0=-3.445419637796856, w1=1.1477899725932328\n",
      "Gradient Descent(603/999): loss=8.990607134958742, w0=-3.4470432763773164, w1=1.1479530846999533\n",
      "Gradient Descent(604/999): loss=8.990343247246239, w0=-3.4486610614295357, w1=1.1481156087562694\n",
      "Gradient Descent(605/999): loss=8.990081258835428, w0=-3.4502730140566054, w1=1.1482775468822162\n",
      "Gradient Descent(606/999): loss=8.989821156056305, w0=-3.4518791552855355, w1=1.1484389011901845\n",
      "Gradient Descent(607/999): loss=8.989562925337252, w0=-3.4534795060675303, w1=1.1485996737849502\n",
      "Gradient Descent(608/999): loss=8.98930655320433, w0=-3.4550740872782604, w1=1.1487598667637013\n",
      "Gradient Descent(609/999): loss=8.989052026280575, w0=-3.4566629197181364, w1=1.1489194822160635\n",
      "Gradient Descent(610/999): loss=8.988799331285307, w0=-3.4582460241125785, w1=1.1490785222241315\n",
      "Gradient Descent(611/999): loss=8.988548455033433, w0=-3.4598234211122887, w1=1.1492369888624911\n",
      "Gradient Descent(612/999): loss=8.988299384434749, w0=-3.4613951312935183, w1=1.1493948841982509\n",
      "Gradient Descent(613/999): loss=8.988052106493274, w0=-3.462961175158338, w1=1.1495522102910654\n",
      "Gradient Descent(614/999): loss=8.987806608306563, w0=-3.464521573134904, w1=1.1497089691931652\n",
      "Gradient Descent(615/999): loss=8.987562877065038, w0=-3.4660763455777257, w1=1.149865162949381\n",
      "Gradient Descent(616/999): loss=8.987320900051307, w0=-3.4676255127679307, w1=1.1500207935971725\n",
      "Gradient Descent(617/999): loss=8.987080664639514, w0=-3.4691690949135285, w1=1.1501758631666532\n",
      "Gradient Descent(618/999): loss=8.986842158294687, w0=-3.4707071121496753, w1=1.1503303736806179\n",
      "Gradient Descent(619/999): loss=8.986605368572054, w0=-3.472239584538936, w1=1.150484327154569\n",
      "Gradient Descent(620/999): loss=8.986370283116429, w0=-3.4737665320715463, w1=1.1506377255967424\n",
      "Gradient Descent(621/999): loss=8.98613688966154, w0=-3.4752879746656733, w1=1.1507905710081343\n",
      "Gradient Descent(622/999): loss=8.98590517602941, w0=-3.4768039321676754, w1=1.150942865382526\n",
      "Gradient Descent(623/999): loss=8.985675130129701, w0=-3.478314424352361, w1=1.1510946107065123\n",
      "Gradient Descent(624/999): loss=8.985446739959105, w0=-3.4798194709232457, w1=1.1512458089595248\n",
      "Gradient Descent(625/999): loss=8.985219993600698, w0=-3.4813190915128116, w1=1.1513964621138588\n",
      "Gradient Descent(626/999): loss=8.984994879223326, w0=-3.482813305682761, w1=1.1515465721347002\n",
      "Gradient Descent(627/999): loss=8.984771385080997, w0=-3.4843021329242725, w1=1.151696140980149\n",
      "Gradient Descent(628/999): loss=8.98454949951225, w0=-3.4857855926582557, w1=1.1518451706012458\n",
      "Gradient Descent(629/999): loss=8.984329210939563, w0=-3.4872637042356036, w1=1.1519936629419987\n",
      "Gradient Descent(630/999): loss=8.984110507868737, w0=-3.488736486937446, w1=1.152141619939405\n",
      "Gradient Descent(631/999): loss=8.983893378888306, w0=-3.4902039599754007, w1=1.1522890435234816\n",
      "Gradient Descent(632/999): loss=8.983677812668933, w0=-3.4916661424918227, w1=1.1524359356172842\n",
      "Gradient Descent(633/999): loss=8.983463797962823, w0=-3.4931230535600566, w1=1.1525822981369385\n",
      "Gradient Descent(634/999): loss=8.983251323603135, w0=-3.4945747121846833, w1=1.1527281329916597\n",
      "Gradient Descent(635/999): loss=8.983040378503404, w0=-3.496021137301769, w1=1.1528734420837816\n",
      "Gradient Descent(636/999): loss=8.982830951656956, w0=-3.4974623477791105, w1=1.1530182273087792\n",
      "Gradient Descent(637/999): loss=8.982623032136333, w0=-3.498898362416484, w1=1.1531624905552933\n",
      "Gradient Descent(638/999): loss=8.982416609092734, w0=-3.500329199945888, w1=1.153306233705157\n",
      "Gradient Descent(639/999): loss=8.982211671755431, w0=-3.501754879031789, w1=1.153449458633418\n",
      "Gradient Descent(640/999): loss=8.982008209431227, w0=-3.503175418271365, w1=1.1535921672083647\n",
      "Gradient Descent(641/999): loss=8.981806211503882, w0=-3.5045908361947458, w1=1.1537343612915496\n",
      "Gradient Descent(642/999): loss=8.981605667433568, w0=-3.506001151265259, w1=1.153876042737814\n",
      "Gradient Descent(643/999): loss=8.981406566756311, w0=-3.507406381879666, w1=1.154017213395313\n",
      "Gradient Descent(644/999): loss=8.981208899083459, w0=-3.5088065463684064, w1=1.1541578751055366\n",
      "Gradient Descent(645/999): loss=8.981012654101125, w0=-3.5102016629958337, w1=1.154298029703339\n",
      "Gradient Descent(646/999): loss=8.980817821569655, w0=-3.511591749960455, w1=1.1544376790169557\n",
      "Gradient Descent(647/999): loss=8.980624391323095, w0=-3.512976825395169, w1=1.154576824868035\n",
      "Gradient Descent(648/999): loss=8.98043235326866, w0=-3.514356907367502, w1=1.1547154690716541\n",
      "Gradient Descent(649/999): loss=8.980241697386207, w0=-3.5157320138798416, w1=1.1548536134363496\n",
      "Gradient Descent(650/999): loss=8.980052413727709, w0=-3.5171021628696755, w1=1.1549912597641356\n",
      "Gradient Descent(651/999): loss=8.979864492416741, w0=-3.518467372209822, w1=1.1551284098505314\n",
      "Gradient Descent(652/999): loss=8.97967792364796, w0=-3.519827659708665, w1=1.1552650654845817\n",
      "Gradient Descent(653/999): loss=8.979492697686608, w0=-3.5211830431103857, w1=1.1554012284488824\n",
      "Gradient Descent(654/999): loss=8.97930880486797, w0=-3.522533540095194, w1=1.155536900519602\n",
      "Gradient Descent(655/999): loss=8.979126235596912, w0=-3.523879168279559, w1=1.1556720834665062\n",
      "Gradient Descent(656/999): loss=8.978944980347341, w0=-3.52521994521644, w1=1.1558067790529798\n",
      "Gradient Descent(657/999): loss=8.97876502966175, w0=-3.5265558883955137, w1=1.155940989036051\n",
      "Gradient Descent(658/999): loss=8.978586374150677, w0=-3.5278870152434028, w1=1.1560747151664128\n",
      "Gradient Descent(659/999): loss=8.978409004492258, w0=-3.529213343123905, w1=1.1562079591884469\n",
      "Gradient Descent(660/999): loss=8.978232911431718, w0=-3.5305348893382167, w1=1.1563407228402465\n",
      "Gradient Descent(661/999): loss=8.97805808578088, w0=-3.5318516711251613, w1=1.1564730078536383\n",
      "Gradient Descent(662/999): loss=8.977884518417714, w0=-3.5331637056614125, w1=1.1566048159542048\n",
      "Gradient Descent(663/999): loss=8.977712200285836, w0=-3.534471010061719, w1=1.1567361488613093\n",
      "Gradient Descent(664/999): loss=8.977541122394047, w0=-3.5357736013791268, w1=1.1568670082881145\n",
      "Gradient Descent(665/999): loss=8.977371275815862, w0=-3.5370714966052037, w1=1.1569973959416078\n",
      "Gradient Descent(666/999): loss=8.97720265168904, w0=-3.5383647126702584, w1=1.1571273135226228\n",
      "Gradient Descent(667/999): loss=8.977035241215127, w0=-3.539653266443563, w1=1.1572567627258596\n",
      "Gradient Descent(668/999): loss=8.976869035658996, w0=-3.5409371747335734, w1=1.1573857452399117\n",
      "Gradient Descent(669/999): loss=8.976704026348386, w0=-3.542216454288147, w1=1.1575142627472816\n",
      "Gradient Descent(670/999): loss=8.976540204673457, w0=-3.5434911217947613, w1=1.157642316924408\n",
      "Gradient Descent(671/999): loss=8.97637756208634, w0=-3.544761193880734, w1=1.1577699094416856\n",
      "Gradient Descent(672/999): loss=8.976216090100682, w0=-3.5460266871134367, w1=1.1578970419634858\n",
      "Gradient Descent(673/999): loss=8.976055780291214, w0=-3.5472876180005133, w1=1.1580237161481812\n",
      "Gradient Descent(674/999): loss=8.975896624293302, w0=-3.5485440029900936, w1=1.1581499336481644\n",
      "Gradient Descent(675/999): loss=8.975738613802523, w0=-3.54979585847101, w1=1.1582756961098717\n",
      "Gradient Descent(676/999): loss=8.97558174057422, w0=-3.5510432007730084, w1=1.1584010051738027\n",
      "Gradient Descent(677/999): loss=8.975425996423077, w0=-3.552286046166964, w1=1.1585258624745445\n",
      "Gradient Descent(678/999): loss=8.975271373222684, w0=-3.5535244108650925, w1=1.158650269640788\n",
      "Gradient Descent(679/999): loss=8.975117862905135, w0=-3.554758311021161, w1=1.1587742282953564\n",
      "Gradient Descent(680/999): loss=8.974965457460574, w0=-3.555987762730699, w1=1.158897740055219\n",
      "Gradient Descent(681/999): loss=8.974814148936806, w0=-3.557212782031209, w1=1.159020806531517\n",
      "Gradient Descent(682/999): loss=8.974663929438874, w0=-3.5584333849023744, w1=1.1591434293295837\n",
      "Gradient Descent(683/999): loss=8.974514791128632, w0=-3.5596495872662697, w1=1.1592656100489631\n",
      "Gradient Descent(684/999): loss=8.974366726224359, w0=-3.560861404987567, w1=1.1593873502834344\n",
      "Gradient Descent(685/999): loss=8.974219727000337, w0=-3.5620688538737433, w1=1.1595086516210296\n",
      "Gradient Descent(686/999): loss=8.974073785786457, w0=-3.5632719496752863, w1=1.1596295156440564\n",
      "Gradient Descent(687/999): loss=8.973928894967807, w0=-3.5644707080859, w1=1.1597499439291175\n",
      "Gradient Descent(688/999): loss=8.973785046984288, w0=-3.565665144742711, w1=1.1598699380471316\n",
      "Gradient Descent(689/999): loss=8.973642234330216, w0=-3.5668552752264686, w1=1.1599894995633546\n",
      "Gradient Descent(690/999): loss=8.97350044955392, w0=-3.5680411150617526, w1=1.1601086300373982\n",
      "Gradient Descent(691/999): loss=8.973359685257375, w0=-3.569222679717173, w1=1.1602273310232527\n",
      "Gradient Descent(692/999): loss=8.973219934095788, w0=-3.5703999846055727, w1=1.1603456040693045\n",
      "Gradient Descent(693/999): loss=8.973081188777238, w0=-3.571573045084228, w1=1.1604634507183598\n",
      "Gradient Descent(694/999): loss=8.972943442062286, w0=-3.5727418764550487, w1=1.1605808725076605\n",
      "Gradient Descent(695/999): loss=8.972806686763594, w0=-3.57390649396478, w1=1.1606978709689075\n",
      "Gradient Descent(696/999): loss=8.972670915745557, w0=-3.5750669128051986, w1=1.1608144476282802\n",
      "Gradient Descent(697/999): loss=8.972536121923927, w0=-3.5762231481133115, w1=1.1609306040064546\n",
      "Gradient Descent(698/999): loss=8.972402298265449, w0=-3.5773752149715548, w1=1.1610463416186252\n",
      "Gradient Descent(699/999): loss=8.972269437787476, w0=-3.578523128407989, w1=1.161161661974524\n",
      "Gradient Descent(700/999): loss=8.972137533557634, w0=-3.5796669033964954, w1=1.1612765665784388\n",
      "Gradient Descent(701/999): loss=8.972006578693431, w0=-3.5808065548569727, w1=1.1613910569292365\n",
      "Gradient Descent(702/999): loss=8.97187656636192, w0=-3.581942097655529, w1=1.161505134520378\n",
      "Gradient Descent(703/999): loss=8.971747489779329, w0=-3.583073546604678, w1=1.1616188008399415\n",
      "Gradient Descent(704/999): loss=8.971619342210708, w0=-3.584200916463532, w1=1.1617320573706393\n",
      "Gradient Descent(705/999): loss=8.97149211696959, w0=-3.5853242219379924, w1=1.1618449055898399\n",
      "Gradient Descent(706/999): loss=8.971365807417627, w0=-3.586443477680944, w1=1.161957346969583\n",
      "Gradient Descent(707/999): loss=8.971240406964247, w0=-3.5875586982924457, w1=1.162069382976604\n",
      "Gradient Descent(708/999): loss=8.97111590906632, w0=-3.588669898319919, w1=1.1621810150723486\n",
      "Gradient Descent(709/999): loss=8.970992307227805, w0=-3.5897770922583394, w1=1.1622922447129946\n",
      "Gradient Descent(710/999): loss=8.970869594999417, w0=-3.5908802945504266, w1=1.1624030733494697\n",
      "Gradient Descent(711/999): loss=8.970747765978288, w0=-3.5919795195868303, w1=1.162513502427471\n",
      "Gradient Descent(712/999): loss=8.970626813807637, w0=-3.5930747817063193, w1=1.162623533387483\n",
      "Gradient Descent(713/999): loss=8.97050673217643, w0=-3.594166095195969, w1=1.1627331676647976\n",
      "Gradient Descent(714/999): loss=8.970387514819063, w0=-3.595253474291346, w1=1.162842406689532\n",
      "Gradient Descent(715/999): loss=8.970269155515014, w0=-3.596336933176696, w1=1.1629512518866474\n",
      "Gradient Descent(716/999): loss=8.970151648088553, w0=-3.5974164859851276, w1=1.1630597046759674\n",
      "Gradient Descent(717/999): loss=8.970034986408384, w0=-3.5984921467987965, w1=1.1631677664721984\n",
      "Gradient Descent(718/999): loss=8.969919164387342, w0=-3.5995639296490896, w1=1.1632754386849442\n",
      "Gradient Descent(719/999): loss=8.96980417598208, w0=-3.6006318485168083, w1=1.1633827227187288\n",
      "Gradient Descent(720/999): loss=8.96969001519274, w0=-3.60169591733235, w1=1.1634896199730111\n",
      "Gradient Descent(721/999): loss=8.969576676062658, w0=-3.6027561499758907, w1=1.163596131842206\n",
      "Gradient Descent(722/999): loss=8.969464152678027, w0=-3.6038125602775657, w1=1.1637022597156998\n",
      "Gradient Descent(723/999): loss=8.969352439167617, w0=-3.6048651620176497, w1=1.1638080049778714\n",
      "Gradient Descent(724/999): loss=8.969241529702447, w0=-3.6059139689267377, w1=1.1639133690081074\n",
      "Gradient Descent(725/999): loss=8.969131418495497, w0=-3.606958994685922, w1=1.1640183531808221\n",
      "Gradient Descent(726/999): loss=8.969022099801393, w0=-3.6080002529269732, w1=1.164122958865475\n",
      "Gradient Descent(727/999): loss=8.968913567916115, w0=-3.609037757232516, w1=1.1642271874265877\n",
      "Gradient Descent(728/999): loss=8.968805817176692, w0=-3.6100715211362076, w1=1.1643310402237628\n",
      "Gradient Descent(729/999): loss=8.968698841960922, w0=-3.611101558122913, w1=1.1644345186117016\n",
      "Gradient Descent(730/999): loss=8.968592636687061, w0=-3.612127881628882, w1=1.164537623940221\n",
      "Gradient Descent(731/999): loss=8.968487195813536, w0=-3.613150505041925, w1=1.1646403575542716\n",
      "Gradient Descent(732/999): loss=8.968382513838662, w0=-3.6141694417015855, w1=1.1647427207939556\n",
      "Gradient Descent(733/999): loss=8.96827858530036, w0=-3.6151847048993164, w1=1.1648447149945433\n",
      "Gradient Descent(734/999): loss=8.96817540477585, w0=-3.616196307878652, w1=1.1649463414864913\n",
      "Gradient Descent(735/999): loss=8.968072966881392, w0=-3.6172042638353803, w1=1.16504760159546\n",
      "Gradient Descent(736/999): loss=8.96797126627199, w0=-3.6182085859177175, w1=1.1651484966423302\n",
      "Gradient Descent(737/999): loss=8.967870297641115, w0=-3.619209287226477, w1=1.1652490279432204\n",
      "Gradient Descent(738/999): loss=8.967770055720438, w0=-3.620206380815241, w1=1.165349196809505\n",
      "Gradient Descent(739/999): loss=8.967670535279547, w0=-3.6211998796905327, w1=1.16544900454783\n",
      "Gradient Descent(740/999): loss=8.967571731125672, w0=-3.622189796811982, w1=1.165548452460131\n",
      "Gradient Descent(741/999): loss=8.967473638103417, w0=-3.623176145092498, w1=1.1656475418436496\n",
      "Gradient Descent(742/999): loss=8.967376251094498, w0=-3.624158937398436, w1=1.1657462739909508\n",
      "Gradient Descent(743/999): loss=8.967279565017458, w0=-3.625138186549767, w1=1.1658446501899398\n",
      "Gradient Descent(744/999): loss=8.967183574827427, w0=-3.626113905320241, w1=1.1659426717238783\n",
      "Gradient Descent(745/999): loss=8.96708827551583, w0=-3.6270861064375586, w1=1.1660403398714019\n",
      "Gradient Descent(746/999): loss=8.966993662110156, w0=-3.628054802583533, w1=1.1661376559065364\n",
      "Gradient Descent(747/999): loss=8.96689972967367, w0=-3.6290200063942577, w1=1.166234621098714\n",
      "Gradient Descent(748/999): loss=8.966806473305178, w0=-3.6299817304602704, w1=1.1663312367127916\n",
      "Gradient Descent(749/999): loss=8.966713888138756, w0=-3.630939987326718, w1=1.1664275040090644\n",
      "Gradient Descent(750/999): loss=8.966621969343501, w0=-3.631894789493519, w1=1.1665234242432854\n",
      "Gradient Descent(751/999): loss=8.966530712123287, w0=-3.6328461494155277, w1=1.1666189986666795\n",
      "Gradient Descent(752/999): loss=8.966440111716501, w0=-3.633794079502697, w1=1.1667142285259613\n",
      "Gradient Descent(753/999): loss=8.9663501633958, w0=-3.6347385921202378, w1=1.1668091150633502\n",
      "Gradient Descent(754/999): loss=8.966260862467877, w0=-3.6356796995887835, w1=1.1669036595165876\n",
      "Gradient Descent(755/999): loss=8.966172204273189, w0=-3.636617414184549, w1=1.1669978631189522\n",
      "Gradient Descent(756/999): loss=8.966084184185739, w0=-3.637551748139491, w1=1.1670917270992769\n",
      "Gradient Descent(757/999): loss=8.965996797612824, w0=-3.638482713641467, w1=1.1671852526819642\n",
      "Gradient Descent(758/999): loss=8.965910039994794, w0=-3.6394103228343955, w1=1.1672784410870025\n",
      "Gradient Descent(759/999): loss=8.96582390680482, w0=-3.640334587818414, w1=1.1673712935299816\n",
      "Gradient Descent(760/999): loss=8.965738393548651, w0=-3.641255520650037, w1=1.1674638112221094\n",
      "Gradient Descent(761/999): loss=8.965653495764384, w0=-3.642173133342312, w1=1.1675559953702266\n",
      "Gradient Descent(762/999): loss=8.965569209022231, w0=-3.6430874378649776, w1=1.1676478471768237\n",
      "Gradient Descent(763/999): loss=8.965485528924285, w0=-3.6439984461446193, w1=1.1677393678400552\n",
      "Gradient Descent(764/999): loss=8.965402451104296, w0=-3.6449061700648246, w1=1.1678305585537563\n",
      "Gradient Descent(765/999): loss=8.965319971227434, w0=-3.6458106214663393, w1=1.1679214205074586\n",
      "Gradient Descent(766/999): loss=8.965238084990073, w0=-3.64671181214722, w1=1.168011954886405\n",
      "Gradient Descent(767/999): loss=8.965156788119561, w0=-3.6476097538629895, w1=1.1681021628715647\n",
      "Gradient Descent(768/999): loss=8.965076076373997, w0=-3.64850445832679, w1=1.1681920456396504\n",
      "Gradient Descent(769/999): loss=8.96499594554201, w0=-3.6493959372095346, w1=1.1682816043631317\n",
      "Gradient Descent(770/999): loss=8.964916391442541, w0=-3.6502842021400617, w1=1.1683708402102517\n",
      "Gradient Descent(771/999): loss=8.964837409924625, w0=-3.6511692647052847, w1=1.1684597543450412\n",
      "Gradient Descent(772/999): loss=8.964758996867172, w0=-3.6520511364503445, w1=1.1685483479273353\n",
      "Gradient Descent(773/999): loss=8.964681148178748, w0=-3.6529298288787593, w1=1.1686366221127868\n",
      "Gradient Descent(774/999): loss=8.964603859797379, w0=-3.6538053534525745, w1=1.1687245780528828\n",
      "Gradient Descent(775/999): loss=8.96452712769031, w0=-3.6546777215925133, w1=1.1688122168949584\n",
      "Gradient Descent(776/999): loss=8.964450947853829, w0=-3.655546944678125, w1=1.168899539782213\n",
      "Gradient Descent(777/999): loss=8.964375316313022, w0=-3.6564130340479326, w1=1.1689865478537245\n",
      "Gradient Descent(778/999): loss=8.964300229121605, w0=-3.6572760009995826, w1=1.1690732422444636\n",
      "Gradient Descent(779/999): loss=8.964225682361675, w0=-3.6581358567899906, w1=1.1691596240853097\n",
      "Gradient Descent(780/999): loss=8.964151672143544, w0=-3.658992612635489, w1=1.1692456945030654\n",
      "Gradient Descent(781/999): loss=8.964078194605511, w0=-3.6598462797119735, w1=1.1693314546204698\n",
      "Gradient Descent(782/999): loss=8.964005245913674, w0=-3.6606968691550485, w1=1.1694169055562162\n",
      "Gradient Descent(783/999): loss=8.963932822261722, w0=-3.661544392060172, w1=1.1695020484249627\n",
      "Gradient Descent(784/999): loss=8.963860919870736, w0=-3.662388859482801, w1=1.169586884337351\n",
      "Gradient Descent(785/999): loss=8.963789534989006, w0=-3.663230282438535, w1=1.1696714144000164\n",
      "Gradient Descent(786/999): loss=8.963718663891814, w0=-3.664068671903262, w1=1.1697556397156066\n",
      "Gradient Descent(787/999): loss=8.963648302881259, w0=-3.664904038813297, w1=1.1698395613827928\n",
      "Gradient Descent(788/999): loss=8.963578448286048, w0=-3.6657363940655294, w1=1.169923180496286\n",
      "Gradient Descent(789/999): loss=8.963509096461312, w0=-3.666565748517563, w1=1.1700064981468499\n",
      "Gradient Descent(790/999): loss=8.963440243788419, w0=-3.6673921129878573, w1=1.1700895154213165\n",
      "Gradient Descent(791/999): loss=8.963371886674786, w0=-3.6682154982558695, w1=1.1701722334025986\n",
      "Gradient Descent(792/999): loss=8.963304021553672, w0=-3.6690359150621945, w1=1.1702546531697064\n",
      "Gradient Descent(793/999): loss=8.963236644884027, w0=-3.669853374108706, w1=1.170336775797758\n",
      "Gradient Descent(794/999): loss=8.963169753150268, w0=-3.670667886058695, w1=1.170418602357998\n",
      "Gradient Descent(795/999): loss=8.963103342862132, w0=-3.671479461537009, w1=1.1705001339178063\n",
      "Gradient Descent(796/999): loss=8.963037410554467, w0=-3.6722881111301917, w1=1.1705813715407172\n",
      "Gradient Descent(797/999): loss=8.962971952787061, w0=-3.673093845386619, w1=1.1706623162864283\n",
      "Gradient Descent(798/999): loss=8.962906966144464, w0=-3.6738966748166386, w1=1.170742969210819\n",
      "Gradient Descent(799/999): loss=8.962842447235811, w0=-3.674696609892707, w1=1.1708233313659604\n",
      "Gradient Descent(800/999): loss=8.962778392694643, w0=-3.6754936610495244, w1=1.1709034038001322\n",
      "Gradient Descent(801/999): loss=8.962714799178721, w0=-3.676287838684172, w1=1.1709831875578331\n",
      "Gradient Descent(802/999): loss=8.962651663369876, w0=-3.677079153156249, w1=1.1710626836797977\n",
      "Gradient Descent(803/999): loss=8.96258898197381, w0=-3.6778676147880045, w1=1.1711418932030078\n",
      "Gradient Descent(804/999): loss=8.962526751719937, w0=-3.6786532338644746, w1=1.1712208171607066\n",
      "Gradient Descent(805/999): loss=8.962464969361212, w0=-3.679436020633616, w1=1.171299456582413\n",
      "Gradient Descent(806/999): loss=8.962403631673967, w0=-3.6802159853064396, w1=1.1713778124939334\n",
      "Gradient Descent(807/999): loss=8.962342735457726, w0=-3.680993138057143, w1=1.1714558859173763\n",
      "Gradient Descent(808/999): loss=8.962282277535051, w0=-3.6817674890232444, w1=1.1715336778711656\n",
      "Gradient Descent(809/999): loss=8.962222254751373, w0=-3.6825390483057143, w1=1.1716111893700532\n",
      "Gradient Descent(810/999): loss=8.962162663974834, w0=-3.6833078259691074, w1=1.1716884214251326\n",
      "Gradient Descent(811/999): loss=8.962103502096106, w0=-3.6840738320416935, w1=1.1717653750438528\n",
      "Gradient Descent(812/999): loss=8.962044766028251, w0=-3.6848370765155884, w1=1.1718420512300298\n",
      "Gradient Descent(813/999): loss=8.961986452706538, w0=-3.6855975693468848, w1=1.1719184509838612\n",
      "Gradient Descent(814/999): loss=8.961928559088307, w0=-3.6863553204557813, w1=1.1719945753019387\n",
      "Gradient Descent(815/999): loss=8.961871082152784, w0=-3.687110339726713, w1=1.1720704251772607\n",
      "Gradient Descent(816/999): loss=8.961814018900945, w0=-3.6878626370084793, w1=1.1721460015992462\n",
      "Gradient Descent(817/999): loss=8.96175736635535, w0=-3.6886122221143722, w1=1.172221305553747\n",
      "Gradient Descent(818/999): loss=8.961701121559985, w0=-3.6893591048223064, w1=1.1722963380230598\n",
      "Gradient Descent(819/999): loss=8.961645281580116, w0=-3.6901032948749437, w1=1.1723710999859418\n",
      "Gradient Descent(820/999): loss=8.961589843502132, w0=-3.690844801979823, w1=1.1724455924176198\n",
      "Gradient Descent(821/999): loss=8.961534804433386, w0=-3.6915836358094842, w1=1.1725198162898058\n",
      "Gradient Descent(822/999): loss=8.961480161502056, w0=-3.692319806001598, w1=1.1725937725707078\n",
      "Gradient Descent(823/999): loss=8.96142591185699, w0=-3.6930533221590873, w1=1.1726674622250446\n",
      "Gradient Descent(824/999): loss=8.961372052667551, w0=-3.6937841938502562, w1=1.172740886214055\n",
      "Gradient Descent(825/999): loss=8.961318581123484, w0=-3.694512430608912, w1=1.1728140454955138\n",
      "Gradient Descent(826/999): loss=8.96126549443475, w0=-3.6952380419344917, w1=1.1728869410237428\n",
      "Gradient Descent(827/999): loss=8.961212789831402, w0=-3.695961037292185, w1=1.1729595737496221\n",
      "Gradient Descent(828/999): loss=8.96116046456342, w0=-3.6966814261130567, w1=1.173031944620606\n",
      "Gradient Descent(829/999): loss=8.961108515900582, w0=-3.6973992177941724, w1=1.1731040545807296\n",
      "Gradient Descent(830/999): loss=8.96105694113232, w0=-3.6981144216987176, w1=1.1731759045706285\n",
      "Gradient Descent(831/999): loss=8.961005737567566, w0=-3.6988270471561235, w1=1.1732474955275436\n",
      "Gradient Descent(832/999): loss=8.960954902534624, w0=-3.6995371034621862, w1=1.1733188283853397\n",
      "Gradient Descent(833/999): loss=8.960904433381032, w0=-3.7002445998791886, w1=1.1733899040745126\n",
      "Gradient Descent(834/999): loss=8.960854327473411, w0=-3.7009495456360213, w1=1.1734607235222052\n",
      "Gradient Descent(835/999): loss=8.960804582197346, w0=-3.7016519499283027, w1=1.1735312876522168\n",
      "Gradient Descent(836/999): loss=8.960755194957226, w0=-3.7023518219185, w1=1.1736015973850165\n",
      "Gradient Descent(837/999): loss=8.960706163176129, w0=-3.7030491707360476, w1=1.1736716536377552\n",
      "Gradient Descent(838/999): loss=8.960657484295684, w0=-3.703744005477466, w1=1.173741457324277\n",
      "Gradient Descent(839/999): loss=8.960609155775924, w0=-3.7044363352064815, w1=1.1738110093551315\n",
      "Gradient Descent(840/999): loss=8.960561175095169, w0=-3.705126168954144, w1=1.1738803106375857\n",
      "Gradient Descent(841/999): loss=8.96051353974989, w0=-3.7058135157189445, w1=1.1739493620756356\n",
      "Gradient Descent(842/999): loss=8.96046624725457, w0=-3.7064983844669332, w1=1.1740181645700178\n",
      "Gradient Descent(843/999): loss=8.96041929514159, w0=-3.7071807841318356, w1=1.1740867190182227\n",
      "Gradient Descent(844/999): loss=8.960372680961083, w0=-3.707860723615169, w1=1.1741550263145037\n",
      "Gradient Descent(845/999): loss=8.960326402280824, w0=-3.7085382117863595, w1=1.1742230873498911\n",
      "Gradient Descent(846/999): loss=8.960280456686084, w0=-3.7092132574828574, w1=1.1742909030122022\n",
      "Gradient Descent(847/999): loss=8.960234841779513, w0=-3.7098858695102517, w1=1.1743584741860549\n",
      "Gradient Descent(848/999): loss=8.960189555181033, w0=-3.7105560566423863, w1=1.1744258017528757\n",
      "Gradient Descent(849/999): loss=8.960144594527675, w0=-3.711223827621473, w1=1.1744928865909157\n",
      "Gradient Descent(850/999): loss=8.96009995747349, w0=-3.711889191158207, w1=1.1745597295752581\n",
      "Gradient Descent(851/999): loss=8.960055641689408, w0=-3.7125521559318786, w1=1.174626331577832\n",
      "Gradient Descent(852/999): loss=8.96001164486312, w0=-3.713212730590489, w1=1.1746926934674227\n",
      "Gradient Descent(853/999): loss=8.959967964698974, w0=-3.7138709237508607, w1=1.1747588161096838\n",
      "Gradient Descent(854/999): loss=8.959924598917825, w0=-3.7145267439987517, w1=1.1748247003671477\n",
      "Gradient Descent(855/999): loss=8.959881545256932, w0=-3.715180199888966, w1=1.1748903470992371\n",
      "Gradient Descent(856/999): loss=8.959838801469852, w0=-3.715831299945466, w1=1.1749557571622766\n",
      "Gradient Descent(857/999): loss=8.959796365326302, w0=-3.716480052661484, w1=1.1750209314095037\n",
      "Gradient Descent(858/999): loss=8.959754234612053, w0=-3.7171264664996317, w1=1.1750858706910792\n",
      "Gradient Descent(859/999): loss=8.95971240712881, w0=-3.7177705498920126, w1=1.1751505758540997\n",
      "Gradient Descent(860/999): loss=8.959670880694105, w0=-3.7184123112403302, w1=1.1752150477426067\n",
      "Gradient Descent(861/999): loss=8.959629653141175, w0=-3.7190517589159984, w1=1.1752792871976008\n",
      "Gradient Descent(862/999): loss=8.959588722318852, w0=-3.71968890126025, w1=1.175343295057048\n",
      "Gradient Descent(863/999): loss=8.959548086091452, w0=-3.7203237465842474, w1=1.1754070721558956\n",
      "Gradient Descent(864/999): loss=8.959507742338658, w0=-3.720956303169188, w1=1.1754706193260793\n",
      "Gradient Descent(865/999): loss=8.959467688955424, w0=-3.7215865792664156, w1=1.1755339373965363\n",
      "Gradient Descent(866/999): loss=8.959427923851848, w0=-3.7222145830975246, w1=1.1755970271932146\n",
      "Gradient Descent(867/999): loss=8.959388444953063, w0=-3.72284032285447, w1=1.1756598895390855\n",
      "Gradient Descent(868/999): loss=8.959349250199148, w0=-3.7234638066996735, w1=1.175722525254152\n",
      "Gradient Descent(869/999): loss=8.959310337544999, w0=-3.724085042766129, w1=1.1757849351554626\n",
      "Gradient Descent(870/999): loss=8.959271704960237, w0=-3.7247040391575092, w1=1.1758471200571183\n",
      "Gradient Descent(871/999): loss=8.959233350429098, w0=-3.725320803948273, w1=1.1759090807702859\n",
      "Gradient Descent(872/999): loss=8.95919527195032, w0=-3.725935345183767, w1=1.175970818103209\n",
      "Gradient Descent(873/999): loss=8.959157467537045, w0=-3.7265476708803353, w1=1.1760323328612146\n",
      "Gradient Descent(874/999): loss=8.95911993521672, w0=-3.7271577890254197, w1=1.1760936258467294\n",
      "Gradient Descent(875/999): loss=8.95908267303099, w0=-3.7277657075776665, w1=1.1761546978592845\n",
      "Gradient Descent(876/999): loss=8.959045679035587, w0=-3.728371434467029, w1=1.1762155496955307\n",
      "Gradient Descent(877/999): loss=8.959008951300245, w0=-3.7289749775948726, w1=1.1762761821492453\n",
      "Gradient Descent(878/999): loss=8.958972487908587, w0=-3.7295763448340757, w1=1.1763365960113445\n",
      "Gradient Descent(879/999): loss=8.958936286958028, w0=-3.7301755440291338, w1=1.176396792069893\n",
      "Gradient Descent(880/999): loss=8.958900346559682, w0=-3.7307725829962615, w1=1.1764567711101144\n",
      "Gradient Descent(881/999): loss=8.95886466483825, w0=-3.7313674695234944, w1=1.176516533914401\n",
      "Gradient Descent(882/999): loss=8.958829239931937, w0=-3.7319602113707915, w1=1.1765760812623252\n",
      "Gradient Descent(883/999): loss=8.958794069992342, w0=-3.7325508162701344, w1=1.1766354139306485\n",
      "Gradient Descent(884/999): loss=8.958759153184372, w0=-3.73313929192563, w1=1.1766945326933314\n",
      "Gradient Descent(885/999): loss=8.958724487686146, w0=-3.7337256460136103, w1=1.1767534383215457\n",
      "Gradient Descent(886/999): loss=8.958690071688888, w0=-3.734309886182733, w1=1.176812131583681\n",
      "Gradient Descent(887/999): loss=8.958655903396842, w0=-3.734892020054081, w1=1.1768706132453584\n",
      "Gradient Descent(888/999): loss=8.958621981027179, w0=-3.735472055221261, w1=1.1769288840694376\n",
      "Gradient Descent(889/999): loss=8.958588302809899, w0=-3.736049999250504, w1=1.1769869448160293\n",
      "Gradient Descent(890/999): loss=8.958554866987745, w0=-3.7366258596807627, w1=1.1770447962425021\n",
      "Gradient Descent(891/999): loss=8.958521671816104, w0=-3.737199644023811, w1=1.177102439103496\n",
      "Gradient Descent(892/999): loss=8.958488715562924, w0=-3.7377713597643414, w1=1.177159874150929\n",
      "Gradient Descent(893/999): loss=8.958455996508617, w0=-3.7383410143600617, w1=1.1772171021340094\n",
      "Gradient Descent(894/999): loss=8.958423512945966, w0=-3.7389086152417943, w1=1.177274123799243\n",
      "Gradient Descent(895/999): loss=8.958391263180053, w0=-3.739474169813572, w1=1.177330939890446\n",
      "Gradient Descent(896/999): loss=8.958359245528147, w0=-3.740037685452734, w1=1.1773875511487517\n",
      "Gradient Descent(897/999): loss=8.958327458319637, w0=-3.7405991695100234, w1=1.177443958312622\n",
      "Gradient Descent(898/999): loss=8.958295899895935, w0=-3.7411586293096817, w1=1.1775001621178565\n",
      "Gradient Descent(899/999): loss=8.958264568610383, w0=-3.7417160721495457, w1=1.1775561632976022\n",
      "Gradient Descent(900/999): loss=8.958233462828186, w0=-3.7422715053011424, w1=1.1776119625823627\n",
      "Gradient Descent(901/999): loss=8.958202580926306, w0=-3.742824936009783, w1=1.177667560700008\n",
      "Gradient Descent(902/999): loss=8.958171921293392, w0=-3.743376371494658, w1=1.1777229583757844\n",
      "Gradient Descent(903/999): loss=8.958141482329681, w0=-3.7439258189489317, w1=1.1777781563323229\n",
      "Gradient Descent(904/999): loss=8.958111262446941, w0=-3.744473285539835, w1=1.17783315528965\n",
      "Gradient Descent(905/999): loss=8.95808126006836, w0=-3.74501877840876, w1=1.1778879559651956\n",
      "Gradient Descent(906/999): loss=8.958051473628473, w0=-3.745562304671353, w1=1.1779425590738037\n",
      "Gradient Descent(907/999): loss=8.958021901573089, w0=-3.7461038714176067, w1=1.1779969653277411\n",
      "Gradient Descent(908/999): loss=8.9579925423592, w0=-3.7466434857119526, w1=1.1780511754367065\n",
      "Gradient Descent(909/999): loss=8.957963394454898, w0=-3.7471811545933544, w1=1.17810519010784\n",
      "Gradient Descent(910/999): loss=8.957934456339315, w0=-3.747716885075399, w1=1.1781590100457326\n",
      "Gradient Descent(911/999): loss=8.957905726502515, w0=-3.7482506841463863, w1=1.178212635952435\n",
      "Gradient Descent(912/999): loss=8.957877203445438, w0=-3.7487825587694243, w1=1.1782660685274666\n",
      "Gradient Descent(913/999): loss=8.957848885679805, w0=-3.7493125158825165, w1=1.1783193084678254\n",
      "Gradient Descent(914/999): loss=8.957820771728063, w0=-3.7498405623986537, w1=1.178372356467996\n",
      "Gradient Descent(915/999): loss=8.957792860123279, w0=-3.750366705205904, w1=1.17842521321996\n",
      "Gradient Descent(916/999): loss=8.957765149409084, w0=-3.7508909511675026, w1=1.1784778794132031\n",
      "Gradient Descent(917/999): loss=8.957737638139593, w0=-3.751413307121942, w1=1.178530355734727\n",
      "Gradient Descent(918/999): loss=8.957710324879326, w0=-3.7519337798830596, w1=1.1785826428690547\n",
      "Gradient Descent(919/999): loss=8.957683208203129, w0=-3.7524523762401287, w1=1.178634741498243\n",
      "Gradient Descent(920/999): loss=8.957656286696116, w0=-3.7529691029579455, w1=1.1786866523018886\n",
      "Gradient Descent(921/999): loss=8.95762955895358, w0=-3.7534839667769178, w1=1.1787383759571386\n",
      "Gradient Descent(922/999): loss=8.957603023580916, w0=-3.7539969744131527, w1=1.1787899131386992\n",
      "Gradient Descent(923/999): loss=8.957576679193572, w0=-3.754508132558545, w1=1.1788412645188435\n",
      "Gradient Descent(924/999): loss=8.957550524416948, w0=-3.7550174478808636, w1=1.1788924307674211\n",
      "Gradient Descent(925/999): loss=8.957524557886344, w0=-3.7555249270238384, w1=1.1789434125518672\n",
      "Gradient Descent(926/999): loss=8.957498778246881, w0=-3.756030576607248, w1=1.17899421053721\n",
      "Gradient Descent(927/999): loss=8.957473184153432, w0=-3.756534403227006, w1=1.1790448253860806\n",
      "Gradient Descent(928/999): loss=8.957447774270548, w0=-3.7570364134552445, w1=1.1790952577587208\n",
      "Gradient Descent(929/999): loss=8.957422547272397, w0=-3.757536613840404, w1=1.1791455083129931\n",
      "Gradient Descent(930/999): loss=8.957397501842687, w0=-3.758035010907315, w1=1.1791955777043868\n",
      "Gradient Descent(931/999): loss=8.957372636674599, w0=-3.758531611157286, w1=1.179245466586029\n",
      "Gradient Descent(932/999): loss=8.95734795047072, w0=-3.759026421068186, w1=1.1792951756086922\n",
      "Gradient Descent(933/999): loss=8.957323441942975, w0=-3.759519447094531, w1=1.1793447054208024\n",
      "Gradient Descent(934/999): loss=8.957299109812563, w0=-3.7600106956675656, w1=1.1793940566684478\n",
      "Gradient Descent(935/999): loss=8.957274952809883, w0=-3.7605001731953505, w1=1.1794432299953879\n",
      "Gradient Descent(936/999): loss=8.957250969674472, w0=-3.7609878860628427, w1=1.1794922260430603\n",
      "Gradient Descent(937/999): loss=8.957227159154941, w0=-3.7614738406319814, w1=1.1795410454505912\n",
      "Gradient Descent(938/999): loss=8.95720352000891, w0=-3.7619580432417687, w1=1.1795896888548023\n",
      "Gradient Descent(939/999): loss=8.957180051002931, w0=-3.7624405002083536, w1=1.1796381568902188\n",
      "Gradient Descent(940/999): loss=8.957156750912446, w0=-3.762921217825115, w1=1.1796864501890791\n",
      "Gradient Descent(941/999): loss=8.957133618521702, w0=-3.7634002023627415, w1=1.1797345693813424\n",
      "Gradient Descent(942/999): loss=8.957110652623703, w0=-3.7638774600693163, w1=1.1797825150946952\n",
      "Gradient Descent(943/999): loss=8.957087852020134, w0=-3.764352997170396, w1=1.1798302879545635\n",
      "Gradient Descent(944/999): loss=8.957065215521304, w0=-3.7648268198690933, w1=1.1798778885841164\n",
      "Gradient Descent(945/999): loss=8.957042741946092, w0=-3.765298934346157, w1=1.1799253176042774\n",
      "Gradient Descent(946/999): loss=8.957020430121872, w0=-3.7657693467600537, w1=1.1799725756337314\n",
      "Gradient Descent(947/999): loss=8.956998278884463, w0=-3.766238063247047, w1=1.1800196632889326\n",
      "Gradient Descent(948/999): loss=8.956976287078056, w0=-3.766705089921279, w1=1.1800665811841127\n",
      "Gradient Descent(949/999): loss=8.956954453555168, w0=-3.7671704328748477, w1=1.1801133299312894\n",
      "Gradient Descent(950/999): loss=8.956932777176567, w0=-3.7676340981778895, w1=1.180159910140273\n",
      "Gradient Descent(951/999): loss=8.956911256811228, w0=-3.768096091878656, w1=1.180206322418677\n",
      "Gradient Descent(952/999): loss=8.956889891336262, w0=-3.7685564200035935, w1=1.1802525673719226\n",
      "Gradient Descent(953/999): loss=8.956868679636864, w0=-3.769015088557422, w1=1.1802986456032492\n",
      "Gradient Descent(954/999): loss=8.956847620606252, w0=-3.7694721035232135, w1=1.1803445577137213\n",
      "Gradient Descent(955/999): loss=8.956826713145606, w0=-3.76992747086247, w1=1.1803903043022366\n",
      "Gradient Descent(956/999): loss=8.956805956164027, w0=-3.7703811965152005, w1=1.180435885965533\n",
      "Gradient Descent(957/999): loss=8.956785348578455, w0=-3.7708332864, w1=1.180481303298198\n",
      "Gradient Descent(958/999): loss=8.956764889313623, w0=-3.7712837464141247, w1=1.180526556892675\n",
      "Gradient Descent(959/999): loss=8.95674457730202, w0=-3.7717325824335712, w1=1.1805716473392711\n",
      "Gradient Descent(960/999): loss=8.956724411483798, w0=-3.7721798003131517, w1=1.1806165752261664\n",
      "Gradient Descent(961/999): loss=8.956704390806754, w0=-3.7726254058865702, w1=1.180661341139419\n",
      "Gradient Descent(962/999): loss=8.956684514226248, w0=-3.7730694049664995, w1=1.1807059456629752\n",
      "Gradient Descent(963/999): loss=8.956664780705163, w0=-3.7735118033446566, w1=1.180750389378676\n",
      "Gradient Descent(964/999): loss=8.956645189213841, w0=-3.773952606791878, w1=1.1807946728662644\n",
      "Gradient Descent(965/999): loss=8.956625738730041, w0=-3.7743918210581957, w1=1.1808387967033933\n",
      "Gradient Descent(966/999): loss=8.956606428238882, w0=-3.774829451872911, w1=1.1808827614656336\n",
      "Gradient Descent(967/999): loss=8.956587256732778, w0=-3.7752655049446706, w1=1.1809265677264802\n",
      "Gradient Descent(968/999): loss=8.956568223211402, w0=-3.77569998596154, w1=1.1809702160573616\n",
      "Gradient Descent(969/999): loss=8.956549326681625, w0=-3.7761329005910786, w1=1.181013707027645\n",
      "Gradient Descent(970/999): loss=8.956530566157463, w0=-3.7765642544804128, w1=1.1810570412046462\n",
      "Gradient Descent(971/999): loss=8.956511940660032, w0=-3.77699405325631, w1=1.1811002191536342\n",
      "Gradient Descent(972/999): loss=8.956493449217493, w0=-3.7774223025252525, w1=1.1811432414378416\n",
      "Gradient Descent(973/999): loss=8.956475090865004, w0=-3.7778490078735096, w1=1.1811861086184696\n",
      "Gradient Descent(974/999): loss=8.95645686464466, w0=-3.7782741748672115, w1=1.181228821254696\n",
      "Gradient Descent(975/999): loss=8.956438769605457, w0=-3.778697809052421, w1=1.1812713799036836\n",
      "Gradient Descent(976/999): loss=8.956420804803233, w0=-3.779119915955206, w1=1.181313785120585\n",
      "Gradient Descent(977/999): loss=8.956402969300624, w0=-3.779540501081713, w1=1.1813560374585537\n",
      "Gradient Descent(978/999): loss=8.956385262167009, w0=-3.779959569918237, w1=1.1813981374687454\n",
      "Gradient Descent(979/999): loss=8.956367682478469, w0=-3.7803771279312937, w1=1.1814400857003335\n",
      "Gradient Descent(980/999): loss=8.95635022931773, w0=-3.7807931805676915, w1=1.1814818827005067\n",
      "Gradient Descent(981/999): loss=8.956332901774122, w0=-3.7812077332546017, w1=1.181523529014485\n",
      "Gradient Descent(982/999): loss=8.956315698943532, w0=-3.7816207913996296, w1=1.1815650251855194\n",
      "Gradient Descent(983/999): loss=8.956298619928354, w0=-3.782032360390885, w1=1.1816063717549055\n",
      "Gradient Descent(984/999): loss=8.956281663837439, w0=-3.782442445597053, w1=1.1816475692619848\n",
      "Gradient Descent(985/999): loss=8.95626482978605, w0=-3.782851052367463, w1=1.1816886182441557\n",
      "Gradient Descent(986/999): loss=8.956248116895827, w0=-3.7832581860321595, w1=1.1817295192368795\n",
      "Gradient Descent(987/999): loss=8.956231524294726, w0=-3.78366385190197, w1=1.181770272773686\n",
      "Gradient Descent(988/999): loss=8.956215051116974, w0=-3.7840680552685773, w1=1.1818108793861823\n",
      "Gradient Descent(989/999): loss=8.956198696503042, w0=-3.784470801404585, w1=1.1818513396040584\n",
      "Gradient Descent(990/999): loss=8.956182459599573, w0=-3.7848720955635895, w1=1.1818916539550954\n",
      "Gradient Descent(991/999): loss=8.95616633955937, w0=-3.7852719429802457, w1=1.1819318229651707\n",
      "Gradient Descent(992/999): loss=8.956150335541311, w0=-3.785670348870337, w1=1.181971847158267\n",
      "Gradient Descent(993/999): loss=8.956134446710347, w0=-3.786067318430843, w1=1.1820117270564765\n",
      "Gradient Descent(994/999): loss=8.956118672237432, w0=-3.786462856840007, w1=1.1820514631800105\n",
      "Gradient Descent(995/999): loss=8.956103011299486, w0=-3.7868569692574043, w1=1.1820910560472038\n",
      "Gradient Descent(996/999): loss=8.956087463079355, w0=-3.787249660824008, w1=1.1821305061745233\n",
      "Gradient Descent(997/999): loss=8.95607202676576, w0=-3.7876409366622577, w1=1.1821698140765735\n",
      "Gradient Descent(998/999): loss=8.95605670155327, w0=-3.788030801876125, w1=1.1822089802661035\n",
      "Gradient Descent(999/999): loss=8.956041486642253, w0=-3.788419261551182, w1=1.1822480052540147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.78841926,  1.18224801])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g, cost = gradientDescent(X, y, theta, alpha, iters)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can compute the cost (error) of the trained model using our fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.956041486642253"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the linear model along with the data to visually see how well it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Predicted Profit vs. Population Size')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(data.Population.min(), data.Population.max(), 100)\n",
    "f = g[0] + (g[1] * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(x, f, 'r', label='Prediction')\n",
    "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Profit')\n",
    "ax.set_title('Predicted Profit vs. Population Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good!  Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.  Notice that the cost always decreases - this is an example of a convex optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Error vs. Training Epoch')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(iters), cost, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
